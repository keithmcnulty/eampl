<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Binomial Logistic Regression for Binary Outcomes | Handbook of Regression Modeling in People Analytics: With Examples in R, Python and Julia</title>
  <meta name="description" content="A technical manual of inferential statistics and regression modeling in the people and social sciences" />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Binomial Logistic Regression for Binary Outcomes | Handbook of Regression Modeling in People Analytics: With Examples in R, Python and Julia" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://peopleanalytics-regression-book.org/www/cover/coverpage-og.png" />
  <meta property="og:description" content="A technical manual of inferential statistics and regression modeling in the people and social sciences" />
  <meta name="github-repo" content="keithmcnulty/peopleanalytics-regression-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Binomial Logistic Regression for Binary Outcomes | Handbook of Regression Modeling in People Analytics: With Examples in R, Python and Julia" />
  <meta name="twitter:site" content="@dr_keithmcnulty" />
  <meta name="twitter:description" content="A technical manual of inferential statistics and regression modeling in the people and social sciences" />
  <meta name="twitter:image" content="https://peopleanalytics-regression-book.org/www/cover/coverpage-og.png" />

<meta name="author" content="Keith McNulty" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-reg-ols.html"/>
<link rel="next" href="multinomial-logistic-regression-for-nominal-category-outcomes.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<meta name="citation_title" content="Handbook of Regression Modeling in People Analytics: With Examples in R and Python"/>
<meta name="citation_author" content="Keith McNulty"/>
<meta name="citation_publication_date" content="2021"/>
<meta name="citation_isbn" content="9781003194156"/>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.4/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-N7JZGMVRZK"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-N7JZGMVRZK');
</script>
<link href="https://fonts.googleapis.com/icon?family=Material+Icons"
      rel="stylesheet">


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Handbook of Regression Modeling in People Analytics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#notes-on-data-used-in-this-book"><i class="fa fa-check"></i>Notes on data used in this book</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="foreword-by-alexis-fink.html"><a href="foreword-by-alexis-fink.html"><i class="fa fa-check"></i>Foreword by Alexis Fink</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="inf-model.html"><a href="inf-model.html"><i class="fa fa-check"></i><b>1</b> The Importance of Regression in People Analytics</a>
<ul>
<li class="chapter" data-level="1.1" data-path="inf-model.html"><a href="inf-model.html#why-is-regression-modeling-so-important-in-people-analytics"><i class="fa fa-check"></i><b>1.1</b> Why is regression modeling so important in people analytics?</a></li>
<li class="chapter" data-level="1.2" data-path="inf-model.html"><a href="inf-model.html#theory-modeling"><i class="fa fa-check"></i><b>1.2</b> What do we mean by ‘modeling’‍?</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="inf-model.html"><a href="inf-model.html#the-theory-of-inferential-modeling"><i class="fa fa-check"></i><b>1.2.1</b> The theory of inferential modeling</a></li>
<li class="chapter" data-level="1.2.2" data-path="inf-model.html"><a href="inf-model.html#the-process-of-inferential-modeling"><i class="fa fa-check"></i><b>1.2.2</b> The process of inferential modeling</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="inf-model.html"><a href="inf-model.html#the-structure-system-and-organization-of-this-book"><i class="fa fa-check"></i><b>1.3</b> The structure, system and organization of this book</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html"><i class="fa fa-check"></i><b>2</b> The Basics of the R Programming Language</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#what-is-r"><i class="fa fa-check"></i><b>2.1</b> What is R?</a></li>
<li class="chapter" data-level="2.2" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#how-to-start-using-r"><i class="fa fa-check"></i><b>2.2</b> How to start using R</a></li>
<li class="chapter" data-level="2.3" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#data-in-r"><i class="fa fa-check"></i><b>2.3</b> Data in R</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#data-types"><i class="fa fa-check"></i><b>2.3.1</b> Data types</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#homogeneous-data-structures"><i class="fa fa-check"></i><b>2.3.2</b> Homogeneous data structures</a></li>
<li class="chapter" data-level="2.3.3" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#heterogeneous-data-structures"><i class="fa fa-check"></i><b>2.3.3</b> Heterogeneous data structures</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#working-with-dataframes"><i class="fa fa-check"></i><b>2.4</b> Working with dataframes</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#loading-and-tidying-data-in-dataframes"><i class="fa fa-check"></i><b>2.4.1</b> Loading and tidying data in dataframes</a></li>
<li class="chapter" data-level="2.4.2" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#manipulating-dataframes"><i class="fa fa-check"></i><b>2.4.2</b> Manipulating dataframes</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#functions-packages-and-libraries"><i class="fa fa-check"></i><b>2.5</b> Functions, packages and libraries</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#using-functions"><i class="fa fa-check"></i><b>2.5.1</b> Using functions</a></li>
<li class="chapter" data-level="2.5.2" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#help-with-functions"><i class="fa fa-check"></i><b>2.5.2</b> Help with functions</a></li>
<li class="chapter" data-level="2.5.3" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#writing-your-own-functions"><i class="fa fa-check"></i><b>2.5.3</b> Writing your own functions</a></li>
<li class="chapter" data-level="2.5.4" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#installing-packages"><i class="fa fa-check"></i><b>2.5.4</b> Installing packages</a></li>
<li class="chapter" data-level="2.5.5" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#using-packages"><i class="fa fa-check"></i><b>2.5.5</b> Using packages</a></li>
<li class="chapter" data-level="2.5.6" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#the-pipe-operator"><i class="fa fa-check"></i><b>2.5.6</b> The pipe operator</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#errors-warnings-and-messages"><i class="fa fa-check"></i><b>2.6</b> Errors, warnings and messages</a></li>
<li class="chapter" data-level="2.7" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#plotting-and-graphing"><i class="fa fa-check"></i><b>2.7</b> Plotting and graphing</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#plotting-in-base-r"><i class="fa fa-check"></i><b>2.7.1</b> Plotting in base R</a></li>
<li class="chapter" data-level="2.7.2" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#specialist-plotting-and-graphing-packages"><i class="fa fa-check"></i><b>2.7.2</b> Specialist plotting and graphing packages</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#documenting-your-work-using-r-markdown"><i class="fa fa-check"></i><b>2.8</b> Documenting your work using R Markdown</a></li>
<li class="chapter" data-level="2.9" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#learning-exercises"><i class="fa fa-check"></i><b>2.9</b> Learning exercises</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#discussion-questions"><i class="fa fa-check"></i><b>2.9.1</b> Discussion questions</a></li>
<li class="chapter" data-level="2.9.2" data-path="the-basics-of-the-r-programming-language.html"><a href="the-basics-of-the-r-programming-language.html#data-exercises"><i class="fa fa-check"></i><b>2.9.2</b> Data exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="found-stats.html"><a href="found-stats.html"><i class="fa fa-check"></i><b>3</b> Statistics Foundations</a>
<ul>
<li class="chapter" data-level="3.1" data-path="found-stats.html"><a href="found-stats.html#elementary-descriptive-statistics-of-populations-and-samples"><i class="fa fa-check"></i><b>3.1</b> Elementary descriptive statistics of populations and samples</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="found-stats.html"><a href="found-stats.html#mean-var-sd"><i class="fa fa-check"></i><b>3.1.1</b> Mean, variance and standard deviation</a></li>
<li class="chapter" data-level="3.1.2" data-path="found-stats.html"><a href="found-stats.html#covariance-and-correlation"><i class="fa fa-check"></i><b>3.1.2</b> Covariance and correlation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="found-stats.html"><a href="found-stats.html#distribution-of-random-variables"><i class="fa fa-check"></i><b>3.2</b> Distribution of random variables</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="found-stats.html"><a href="found-stats.html#sampling-of-random-variables"><i class="fa fa-check"></i><b>3.2.1</b> Sampling of random variables</a></li>
<li class="chapter" data-level="3.2.2" data-path="found-stats.html"><a href="found-stats.html#standard-errors-the-t-distribution-and-confidence-intervals"><i class="fa fa-check"></i><b>3.2.2</b> Standard errors, the <span class="math inline">\(t\)</span>-distribution and confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="found-stats.html"><a href="found-stats.html#hyp-tests"><i class="fa fa-check"></i><b>3.3</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="found-stats.html"><a href="found-stats.html#means-sig"><i class="fa fa-check"></i><b>3.3.1</b> Testing for a difference in means (Welch’s <span class="math inline">\(t\)</span>-test)</a></li>
<li class="chapter" data-level="3.3.2" data-path="found-stats.html"><a href="found-stats.html#t-test-cor"><i class="fa fa-check"></i><b>3.3.2</b> Testing for a non-zero correlation between two variables (<span class="math inline">\(t\)</span>-test for correlation)</a></li>
<li class="chapter" data-level="3.3.3" data-path="found-stats.html"><a href="found-stats.html#testing-for-a-difference-in-frequency-distribution-between-different-categories-in-a-data-set-chi-square-test"><i class="fa fa-check"></i><b>3.3.3</b> Testing for a difference in frequency distribution between different categories in a data set (Chi-square test)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="found-stats.html"><a href="found-stats.html#foundational-statistics-in-python"><i class="fa fa-check"></i><b>3.4</b> Foundational statistics in Python</a></li>
<li class="chapter" data-level="3.5" data-path="found-stats.html"><a href="found-stats.html#foundational-statistics-in-julia"><i class="fa fa-check"></i><b>3.5</b> Foundational statistics in Julia</a></li>
<li class="chapter" data-level="3.6" data-path="found-stats.html"><a href="found-stats.html#learning-exercises-1"><i class="fa fa-check"></i><b>3.6</b> Learning exercises</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="found-stats.html"><a href="found-stats.html#discussion-questions-1"><i class="fa fa-check"></i><b>3.6.1</b> Discussion questions</a></li>
<li class="chapter" data-level="3.6.2" data-path="found-stats.html"><a href="found-stats.html#data-exercises-1"><i class="fa fa-check"></i><b>3.6.2</b> Data exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html"><i class="fa fa-check"></i><b>4</b> Linear Regression for Continuous Outcomes</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#when-ols"><i class="fa fa-check"></i><b>4.1</b> When to use it</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#origins-ols"><i class="fa fa-check"></i><b>4.1.1</b> Origins and intuition of linear regression</a></li>
<li class="chapter" data-level="4.1.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#use-cases-ols"><i class="fa fa-check"></i><b>4.1.2</b> Use cases for linear regression</a></li>
<li class="chapter" data-level="4.1.3" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#walkthrough-ols"><i class="fa fa-check"></i><b>4.1.3</b> Walkthrough example</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#simple-ols"><i class="fa fa-check"></i><b>4.2</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#linear-single"><i class="fa fa-check"></i><b>4.2.1</b> Linear relationship between a single input and an outcome</a></li>
<li class="chapter" data-level="4.2.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#minimising-error-ols"><i class="fa fa-check"></i><b>4.2.2</b> Minimising the error</a></li>
<li class="chapter" data-level="4.2.3" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#best-fit-simple-ols"><i class="fa fa-check"></i><b>4.2.3</b> Determining the best fit</a></li>
<li class="chapter" data-level="4.2.4" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#measuring-the-fit-of-the-model"><i class="fa fa-check"></i><b>4.2.4</b> Measuring the fit of the model</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#multiple-linear-regression"><i class="fa fa-check"></i><b>4.3</b> Multiple linear regression</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#running-a-multiple-linear-regression-model-and-interpreting-its-coefficients"><i class="fa fa-check"></i><b>4.3.1</b> Running a multiple linear regression model and interpreting its coefficients</a></li>
<li class="chapter" data-level="4.3.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#coefficient-confidence"><i class="fa fa-check"></i><b>4.3.2</b> Coefficient confidence</a></li>
<li class="chapter" data-level="4.3.3" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#lin-good-fit"><i class="fa fa-check"></i><b>4.3.3</b> Model ‘goodness-of-fit’</a></li>
<li class="chapter" data-level="4.3.4" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#making-predictions-from-your-model"><i class="fa fa-check"></i><b>4.3.4</b> Making predictions from your model</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#managing-inputs-in-linear-regression"><i class="fa fa-check"></i><b>4.4</b> Managing inputs in linear regression</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#relevance-of-input-variables"><i class="fa fa-check"></i><b>4.4.1</b> Relevance of input variables</a></li>
<li class="chapter" data-level="4.4.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#sparseness-missingness-of-data"><i class="fa fa-check"></i><b>4.4.2</b> Sparseness (‘missingness’) of data</a></li>
<li class="chapter" data-level="4.4.3" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#transforming-categorical-inputs-to-dummy-variables"><i class="fa fa-check"></i><b>4.4.3</b> Transforming categorical inputs to dummy variables</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#testing-your-model-assumptions"><i class="fa fa-check"></i><b>4.5</b> Testing your model assumptions</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#assumption-of-linearity-and-additivity"><i class="fa fa-check"></i><b>4.5.1</b> Assumption of linearity and additivity</a></li>
<li class="chapter" data-level="4.5.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#lin-reg-const-var"><i class="fa fa-check"></i><b>4.5.2</b> Assumption of constant error variance</a></li>
<li class="chapter" data-level="4.5.3" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#norm-dist-assum"><i class="fa fa-check"></i><b>4.5.3</b> Assumption of normally distributed errors</a></li>
<li class="chapter" data-level="4.5.4" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#collinearity"><i class="fa fa-check"></i><b>4.5.4</b> Avoiding high collinearity and multicollinearity between input variables</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#extending-multiple-linear-regression"><i class="fa fa-check"></i><b>4.6</b> Extending multiple linear regression</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#interactions-between-input-variables"><i class="fa fa-check"></i><b>4.6.1</b> Interactions between input variables</a></li>
<li class="chapter" data-level="4.6.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#quadratic-and-higher-order-polynomial-terms"><i class="fa fa-check"></i><b>4.6.2</b> Quadratic and higher-order polynomial terms</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#learning-exercises-2"><i class="fa fa-check"></i><b>4.7</b> Learning exercises</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#discussion-questions-2"><i class="fa fa-check"></i><b>4.7.1</b> Discussion questions</a></li>
<li class="chapter" data-level="4.7.2" data-path="linear-reg-ols.html"><a href="linear-reg-ols.html#data-exercises-2"><i class="fa fa-check"></i><b>4.7.2</b> Data exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bin-log-reg.html"><a href="bin-log-reg.html"><i class="fa fa-check"></i><b>5</b> Binomial Logistic Regression for Binary Outcomes</a>
<ul>
<li class="chapter" data-level="5.1" data-path="bin-log-reg.html"><a href="bin-log-reg.html#when-to-use-it"><i class="fa fa-check"></i><b>5.1</b> When to use it</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="bin-log-reg.html"><a href="bin-log-reg.html#logistic-origins"><i class="fa fa-check"></i><b>5.1.1</b> Origins and intuition of binomial logistic regression</a></li>
<li class="chapter" data-level="5.1.2" data-path="bin-log-reg.html"><a href="bin-log-reg.html#use-cases-for-binomial-logistic-regression"><i class="fa fa-check"></i><b>5.1.2</b> Use cases for binomial logistic regression</a></li>
<li class="chapter" data-level="5.1.3" data-path="bin-log-reg.html"><a href="bin-log-reg.html#walkthrough-logit"><i class="fa fa-check"></i><b>5.1.3</b> Walkthrough example</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="bin-log-reg.html"><a href="bin-log-reg.html#mod-prob"><i class="fa fa-check"></i><b>5.2</b> Modeling probabilistic outcomes using a logistic function</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="bin-log-reg.html"><a href="bin-log-reg.html#deriving-the-concept-of-log-odds"><i class="fa fa-check"></i><b>5.2.1</b> Deriving the concept of log odds</a></li>
<li class="chapter" data-level="5.2.2" data-path="bin-log-reg.html"><a href="bin-log-reg.html#modeling-the-log-odds-and-interpreting-the-coefficients"><i class="fa fa-check"></i><b>5.2.2</b> Modeling the log odds and interpreting the coefficients</a></li>
<li class="chapter" data-level="5.2.3" data-path="bin-log-reg.html"><a href="bin-log-reg.html#odds-versus-probability"><i class="fa fa-check"></i><b>5.2.3</b> Odds versus probability</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="bin-log-reg.html"><a href="bin-log-reg.html#running-a-multivariate-binomial-logistic-regression-model"><i class="fa fa-check"></i><b>5.3</b> Running a multivariate binomial logistic regression model</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="bin-log-reg.html"><a href="bin-log-reg.html#running-and-interpreting-a-multivariate-binomial-logistic-regression-model"><i class="fa fa-check"></i><b>5.3.1</b> Running and interpreting a multivariate binomial logistic regression model</a></li>
<li class="chapter" data-level="5.3.2" data-path="bin-log-reg.html"><a href="bin-log-reg.html#logistic-gof"><i class="fa fa-check"></i><b>5.3.2</b> Understanding the fit and goodness-of-fit of a binomial logistic regression model</a></li>
<li class="chapter" data-level="5.3.3" data-path="bin-log-reg.html"><a href="bin-log-reg.html#model-parsimony"><i class="fa fa-check"></i><b>5.3.3</b> Model parsimony</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="bin-log-reg.html"><a href="bin-log-reg.html#other-considerations-in-binomial-logistic-regression"><i class="fa fa-check"></i><b>5.4</b> Other considerations in binomial logistic regression</a></li>
<li class="chapter" data-level="5.5" data-path="bin-log-reg.html"><a href="bin-log-reg.html#learning-exercises-3"><i class="fa fa-check"></i><b>5.5</b> Learning exercises</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="bin-log-reg.html"><a href="bin-log-reg.html#discussion-questions-3"><i class="fa fa-check"></i><b>5.5.1</b> Discussion questions</a></li>
<li class="chapter" data-level="5.5.2" data-path="bin-log-reg.html"><a href="bin-log-reg.html#data-exercises-3"><i class="fa fa-check"></i><b>5.5.2</b> Data exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html"><i class="fa fa-check"></i><b>6</b> Multinomial Logistic Regression for Nominal Category Outcomes</a>
<ul>
<li class="chapter" data-level="6.1" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#when-to-use-it-1"><i class="fa fa-check"></i><b>6.1</b> When to use it</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#intuition-for-multinomial-logistic-regression"><i class="fa fa-check"></i><b>6.1.1</b> Intuition for multinomial logistic regression</a></li>
<li class="chapter" data-level="6.1.2" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#use-cases-for-multinomial-logistic-regression"><i class="fa fa-check"></i><b>6.1.2</b> Use cases for multinomial logistic regression</a></li>
<li class="chapter" data-level="6.1.3" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#walkthrough-example"><i class="fa fa-check"></i><b>6.1.3</b> Walkthrough example</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#stratified"><i class="fa fa-check"></i><b>6.2</b> Running stratified binomial models</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#modeling-the-choice-of-product-a-versus-other-products"><i class="fa fa-check"></i><b>6.2.1</b> Modeling the choice of Product A versus other products</a></li>
<li class="chapter" data-level="6.2.2" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#modeling-other-choices"><i class="fa fa-check"></i><b>6.2.2</b> Modeling other choices</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#running-a-multinomial-regression-model"><i class="fa fa-check"></i><b>6.3</b> Running a multinomial regression model</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#def-ref"><i class="fa fa-check"></i><b>6.3.1</b> Defining a reference level and running the model</a></li>
<li class="chapter" data-level="6.3.2" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#interpreting-the-model"><i class="fa fa-check"></i><b>6.3.2</b> Interpreting the model</a></li>
<li class="chapter" data-level="6.3.3" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#changing-ref"><i class="fa fa-check"></i><b>6.3.3</b> Changing the reference</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#model-simplification-fit-and-goodness-of-fit-for-multinomial-logistic-regression-models"><i class="fa fa-check"></i><b>6.4</b> Model simplification, fit and goodness-of-fit for multinomial logistic regression models</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#elim"><i class="fa fa-check"></i><b>6.4.1</b> Gradual safe elimination of variables</a></li>
<li class="chapter" data-level="6.4.2" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#model-fit-and-goodness-of-fit"><i class="fa fa-check"></i><b>6.4.2</b> Model fit and goodness-of-fit</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#learning-exercises-4"><i class="fa fa-check"></i><b>6.5</b> Learning exercises</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#discussion-questions-4"><i class="fa fa-check"></i><b>6.5.1</b> Discussion questions</a></li>
<li class="chapter" data-level="6.5.2" data-path="multinomial-logistic-regression-for-nominal-category-outcomes.html"><a href="multinomial-logistic-regression-for-nominal-category-outcomes.html#data-exercises-4"><i class="fa fa-check"></i><b>6.5.2</b> Data exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ord-reg.html"><a href="ord-reg.html"><i class="fa fa-check"></i><b>7</b> Proportional Odds Logistic Regression for Ordered Category Outcomes</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ord-reg.html"><a href="ord-reg.html#when-to-use-it-2"><i class="fa fa-check"></i><b>7.1</b> When to use it</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="ord-reg.html"><a href="ord-reg.html#ord-intuit"><i class="fa fa-check"></i><b>7.1.1</b> Intuition for proportional odds logistic regression</a></li>
<li class="chapter" data-level="7.1.2" data-path="ord-reg.html"><a href="ord-reg.html#use-cases-for-proportional-odds-logistic-regression"><i class="fa fa-check"></i><b>7.1.2</b> Use cases for proportional odds logistic regression</a></li>
<li class="chapter" data-level="7.1.3" data-path="ord-reg.html"><a href="ord-reg.html#ord-walkthrough"><i class="fa fa-check"></i><b>7.1.3</b> Walkthrough example</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="ord-reg.html"><a href="ord-reg.html#modeling-ordinal-outcomes-under-the-assumption-of-proportional-odds"><i class="fa fa-check"></i><b>7.2</b> Modeling ordinal outcomes under the assumption of proportional odds</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="ord-reg.html"><a href="ord-reg.html#mod-lin-reg"><i class="fa fa-check"></i><b>7.2.1</b> Using a latent continuous outcome variable to derive a proportional odds model</a></li>
<li class="chapter" data-level="7.2.2" data-path="ord-reg.html"><a href="ord-reg.html#running-a-proportional-odds-logistic-regression-model"><i class="fa fa-check"></i><b>7.2.2</b> Running a proportional odds logistic regression model</a></li>
<li class="chapter" data-level="7.2.3" data-path="ord-reg.html"><a href="ord-reg.html#calculating-the-likelihood-of-an-observation-being-in-a-specific-ordinal-category"><i class="fa fa-check"></i><b>7.2.3</b> Calculating the likelihood of an observation being in a specific ordinal category</a></li>
<li class="chapter" data-level="7.2.4" data-path="ord-reg.html"><a href="ord-reg.html#model-diagnostics"><i class="fa fa-check"></i><b>7.2.4</b> Model diagnostics</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ord-reg.html"><a href="ord-reg.html#testing-the-proportional-odds-assumption"><i class="fa fa-check"></i><b>7.3</b> Testing the proportional odds assumption</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="ord-reg.html"><a href="ord-reg.html#sighting-the-coefficients-of-stratified-binomial-models"><i class="fa fa-check"></i><b>7.3.1</b> Sighting the coefficients of stratified binomial models</a></li>
<li class="chapter" data-level="7.3.2" data-path="ord-reg.html"><a href="ord-reg.html#wald"><i class="fa fa-check"></i><b>7.3.2</b> The Brant-Wald test</a></li>
<li class="chapter" data-level="7.3.3" data-path="ord-reg.html"><a href="ord-reg.html#alternatives-to-proportional-odds-models"><i class="fa fa-check"></i><b>7.3.3</b> Alternatives to proportional odds models</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ord-reg.html"><a href="ord-reg.html#learning-exercises-5"><i class="fa fa-check"></i><b>7.4</b> Learning exercises</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="ord-reg.html"><a href="ord-reg.html#discussion-questions-5"><i class="fa fa-check"></i><b>7.4.1</b> Discussion questions</a></li>
<li class="chapter" data-level="7.4.2" data-path="ord-reg.html"><a href="ord-reg.html#data-exercises-5"><i class="fa fa-check"></i><b>7.4.2</b> Data exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="modeling-explicit-and-latent-hierarchy-in-data.html"><a href="modeling-explicit-and-latent-hierarchy-in-data.html"><i class="fa fa-check"></i><b>8</b> Modeling Explicit and Latent Hierarchy in Data</a>
<ul>
<li class="chapter" data-level="8.1" data-path="modeling-explicit-and-latent-hierarchy-in-data.html"><a href="modeling-explicit-and-latent-hierarchy-in-data.html#mixed"><i class="fa fa-check"></i><b>8.1</b> Mixed models for explicit hierarchy in data</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="modeling-explicit-and-latent-hierarchy-in-data.html"><a href="modeling-explicit-and-latent-hierarchy-in-data.html#fixed-and-random-effects"><i class="fa fa-check"></i><b>8.1.1</b> Fixed and random effects</a></li>
<li class="chapter" data-level="8.1.2" data-path="modeling-explicit-and-latent-hierarchy-in-data.html"><a href="modeling-explicit-and-latent-hierarchy-in-data.html#running-a-mixed-model"><i class="fa fa-check"></i><b>8.1.2</b> Running a mixed model</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="modeling-explicit-and-latent-hierarchy-in-data.html"><a href="modeling-explicit-and-latent-hierarchy-in-data.html#struc-eq-model"><i class="fa fa-check"></i><b>8.2</b> Structural equation models for latent hierarchy in data</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="modeling-explicit-and-latent-hierarchy-in-data.html"><a href="modeling-explicit-and-latent-hierarchy-in-data.html#running-and-assessing-the-measurement-model"><i class="fa fa-check"></i><b>8.2.1</b> Running and assessing the measurement model</a></li>
<li class="chapter" data-level="8.2.2" data-path="modeling-explicit-and-latent-hierarchy-in-data.html"><a href="modeling-explicit-and-latent-hierarchy-in-data.html#running-and-interpreting-the-structural-model"><i class="fa fa-check"></i><b>8.2.2</b> Running and interpreting the structural model</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="modeling-explicit-and-latent-hierarchy-in-data.html"><a href="modeling-explicit-and-latent-hierarchy-in-data.html#learning-exercises-6"><i class="fa fa-check"></i><b>8.3</b> Learning exercises</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="modeling-explicit-and-latent-hierarchy-in-data.html"><a href="modeling-explicit-and-latent-hierarchy-in-data.html#discussion-questions-6"><i class="fa fa-check"></i><b>8.3.1</b> Discussion questions</a></li>
<li class="chapter" data-level="8.3.2" data-path="modeling-explicit-and-latent-hierarchy-in-data.html"><a href="modeling-explicit-and-latent-hierarchy-in-data.html#data-exercises-6"><i class="fa fa-check"></i><b>8.3.2</b> Data exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="survival.html"><a href="survival.html"><i class="fa fa-check"></i><b>9</b> Survival Analysis for Modeling Singular Events Over Time</a>
<ul>
<li class="chapter" data-level="9.1" data-path="survival.html"><a href="survival.html#tracking-and-illustrating-survival-rates-over-the-study-period"><i class="fa fa-check"></i><b>9.1</b> Tracking and illustrating survival rates over the study period</a></li>
<li class="chapter" data-level="9.2" data-path="survival.html"><a href="survival.html#coxphmodel"><i class="fa fa-check"></i><b>9.2</b> Cox proportional hazard regression models</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="survival.html"><a href="survival.html#running-a-cox-proportional-hazard-regression-model"><i class="fa fa-check"></i><b>9.2.1</b> Running a Cox proportional hazard regression model</a></li>
<li class="chapter" data-level="9.2.2" data-path="survival.html"><a href="survival.html#checking-the-proportional-hazard-assumption"><i class="fa fa-check"></i><b>9.2.2</b> Checking the proportional hazard assumption</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="survival.html"><a href="survival.html#frailty-models"><i class="fa fa-check"></i><b>9.3</b> Frailty models</a></li>
<li class="chapter" data-level="9.4" data-path="survival.html"><a href="survival.html#learning-exercises-7"><i class="fa fa-check"></i><b>9.4</b> Learning exercises</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="survival.html"><a href="survival.html#discussion-questions-7"><i class="fa fa-check"></i><b>9.4.1</b> Discussion questions</a></li>
<li class="chapter" data-level="9.4.2" data-path="survival.html"><a href="survival.html#data-exercises-7"><i class="fa fa-check"></i><b>9.4.2</b> Data exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="alt-approaches.html"><a href="alt-approaches.html"><i class="fa fa-check"></i><b>10</b> Alternative Technical Approaches in R, Python and Julia</a>
<ul>
<li class="chapter" data-level="10.1" data-path="alt-approaches.html"><a href="alt-approaches.html#tidier-modeling-approaches-in-r"><i class="fa fa-check"></i><b>10.1</b> ‘Tidier’ modeling approaches in R</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="alt-approaches.html"><a href="alt-approaches.html#the-broom-package"><i class="fa fa-check"></i><b>10.1.1</b> The <code>broom</code> package</a></li>
<li class="chapter" data-level="10.1.2" data-path="alt-approaches.html"><a href="alt-approaches.html#the-parsnip-package"><i class="fa fa-check"></i><b>10.1.2</b> The <code>parsnip</code> package</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="alt-approaches.html"><a href="alt-approaches.html#inferential-statistical-modeling-in-python"><i class="fa fa-check"></i><b>10.2</b> Inferential statistical modeling in Python</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="alt-approaches.html"><a href="alt-approaches.html#ordinary-least-squares-ols-linear-regression"><i class="fa fa-check"></i><b>10.2.1</b> Ordinary Least Squares (OLS) linear regression</a></li>
<li class="chapter" data-level="10.2.2" data-path="alt-approaches.html"><a href="alt-approaches.html#binomial-logistic-regression"><i class="fa fa-check"></i><b>10.2.2</b> Binomial logistic regression</a></li>
<li class="chapter" data-level="10.2.3" data-path="alt-approaches.html"><a href="alt-approaches.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>10.2.3</b> Multinomial logistic regression</a></li>
<li class="chapter" data-level="10.2.4" data-path="alt-approaches.html"><a href="alt-approaches.html#ordinal-logistic-regression"><i class="fa fa-check"></i><b>10.2.4</b> Ordinal logistic regression</a></li>
<li class="chapter" data-level="10.2.5" data-path="alt-approaches.html"><a href="alt-approaches.html#mixed-effects-models"><i class="fa fa-check"></i><b>10.2.5</b> Mixed effects models</a></li>
<li class="chapter" data-level="10.2.6" data-path="alt-approaches.html"><a href="alt-approaches.html#structural-equation-models"><i class="fa fa-check"></i><b>10.2.6</b> Structural equation models</a></li>
<li class="chapter" data-level="10.2.7" data-path="alt-approaches.html"><a href="alt-approaches.html#survival-analysis"><i class="fa fa-check"></i><b>10.2.7</b> Survival analysis</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="alt-approaches.html"><a href="alt-approaches.html#inferential-statistical-modeling-in-julia"><i class="fa fa-check"></i><b>10.3</b> Inferential statistical modeling in Julia</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="alt-approaches.html"><a href="alt-approaches.html#ordinary-least-squares-ols-linear-regression-1"><i class="fa fa-check"></i><b>10.3.1</b> Ordinary Least Squares (OLS) linear regression</a></li>
<li class="chapter" data-level="10.3.2" data-path="alt-approaches.html"><a href="alt-approaches.html#binomial-logistic-regression-1"><i class="fa fa-check"></i><b>10.3.2</b> Binomial logistic regression</a></li>
<li class="chapter" data-level="10.3.3" data-path="alt-approaches.html"><a href="alt-approaches.html#multinomial-logistic-regression-1"><i class="fa fa-check"></i><b>10.3.3</b> Multinomial logistic regression</a></li>
<li class="chapter" data-level="10.3.4" data-path="alt-approaches.html"><a href="alt-approaches.html#proportional-odds-logistic-regression"><i class="fa fa-check"></i><b>10.3.4</b> Proportional odds logistic regression</a></li>
<li class="chapter" data-level="10.3.5" data-path="alt-approaches.html"><a href="alt-approaches.html#mixed-models"><i class="fa fa-check"></i><b>10.3.5</b> Mixed models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="power-tests.html"><a href="power-tests.html"><i class="fa fa-check"></i><b>11</b> Power Analysis to Estimate Required Sample Sizes for Modeling</a>
<ul>
<li class="chapter" data-level="11.1" data-path="power-tests.html"><a href="power-tests.html#errors-effect-sizes-and-statistical-power"><i class="fa fa-check"></i><b>11.1</b> Errors, effect sizes and statistical power</a></li>
<li class="chapter" data-level="11.2" data-path="power-tests.html"><a href="power-tests.html#simple-stats"><i class="fa fa-check"></i><b>11.2</b> Power analysis for simple hypothesis tests</a></li>
<li class="chapter" data-level="11.3" data-path="power-tests.html"><a href="power-tests.html#power-analysis-for-linear-regression-models"><i class="fa fa-check"></i><b>11.3</b> Power analysis for linear regression models</a></li>
<li class="chapter" data-level="11.4" data-path="power-tests.html"><a href="power-tests.html#power-analysis-for-log-likelihood-regression-models"><i class="fa fa-check"></i><b>11.4</b> Power analysis for log-likelihood regression models</a></li>
<li class="chapter" data-level="11.5" data-path="power-tests.html"><a href="power-tests.html#power-analysis-for-hierarchical-regression-models"><i class="fa fa-check"></i><b>11.5</b> Power analysis for hierarchical regression models</a></li>
<li class="chapter" data-level="11.6" data-path="power-tests.html"><a href="power-tests.html#power-analysis-using-python"><i class="fa fa-check"></i><b>11.6</b> Power analysis using Python</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="solutions-to-exercises-slide-presentations-videos-and-other-learning-resources.html"><a href="solutions-to-exercises-slide-presentations-videos-and-other-learning-resources.html"><i class="fa fa-check"></i>Solutions to Exercises, Slide Presentations, Videos and Other Learning Resources</a>
<ul>
<li class="chapter" data-level="" data-path="solutions-to-exercises-slide-presentations-videos-and-other-learning-resources.html"><a href="solutions-to-exercises-slide-presentations-videos-and-other-learning-resources.html#solutions-to-exercises"><i class="fa fa-check"></i>Solutions to exercises</a></li>
<li class="chapter" data-level="" data-path="solutions-to-exercises-slide-presentations-videos-and-other-learning-resources.html"><a href="solutions-to-exercises-slide-presentations-videos-and-other-learning-resources.html#learning-resources"><i class="fa fa-check"></i>Learning resources</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Handbook of Regression Modeling in People Analytics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bin-log-reg" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">5</span> Binomial Logistic Regression for Binary Outcomes<a href="bin-log-reg.html#bin-log-reg" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In the previous chapter we looked at how to explain outcomes that have continuous scale, such as quantity, money, height or weight. While there are a number of typical outcomes of this type in the people analytics domain, they are not the most common form of outcomes that are typically modeled. Much more common are situations where the outcome of interest takes the form of a limited set of classes. Binary (two class) problems are very common. Hiring, promotion and attrition are often modeled as binary outcomes: for example ‘Promoted’ or ‘Not promoted’. Multi-class outcomes like performance ratings on an ordinal scale, or survey responses on a Likert scale are often converted to binary outcomes by dividing the ratings into two groups, for example ‘High’ and ‘Not High’.</p>
<p>In any situation where our outcome is binary, we are effectively working with likelihoods. These are not generally linear in nature, and so we no longer have the comfort of our inputs being <em>directly</em> linearly related to our outcome. Therefore direct linear regression methods such as Ordinary Least Squares regression are not well suited to outcomes of this type. Instead, linear relationships can be inferred on <em>transformations</em> of the outcome variable, which gives us a path to building interpretable models. Hence, binomial logistic regression is said to be in a class of <em>generalized linear models</em> or <em>GLMs</em>. Understanding logistic regression and using it reliably in practice is not straightforward, but it is an invaluable skill to have in the people analytics domain. The mathematics of this chapter is a little more involved but worth the time investment in order to build a competent understanding of how to interpret these types of models.</p>
<div id="when-to-use-it" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> When to use it<a href="bin-log-reg.html#when-to-use-it" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="logistic-origins" class="section level3 hasAnchor" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Origins and intuition of binomial logistic regression<a href="bin-log-reg.html#logistic-origins" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <em>logistic function</em> was first introduced by the Belgian mathematician Pierre François Verhulst in the mid-1800s as a tool for modeling population growth for humans, animals and certain species of plants and fruits. By this time, it was generally accepted that population growth could not continue exponentially forever, and that there were environmental and resource limits which place a maximum limit on the size of a population. The formula for Verhulst’s function was:</p>
<p><span class="math display">\[
y = \frac{L}{1 + e^{-k(x - x_0)}}
\]</span>
where <span class="math inline">\(e\)</span> is the exponential constant, <span class="math inline">\(x_0\)</span> is the value of <span class="math inline">\(x\)</span> at the midpoint, <span class="math inline">\(L\)</span> is the maximum value of <span class="math inline">\(y\)</span> (known as the ‘carrying capacity’) and <span class="math inline">\(k\)</span> is the maximum gradient of the curve.</p>
<p>The logistic function, as shown in Figure <a href="bin-log-reg.html#fig:logistic-function-verhulst">5.1</a>, was felt to accurately capture the theorized stages of population growth, with slower growth in the initial stage, moving to exponential growth during the intermediate stage and then to slower growth as the population approaches its carrying capacity.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:logistic-function-verhulst"></span>
<img src="www/05/logistic-curve.png" alt="Verhulst's logistic function modeled both the exponential nature and the natural limit of population growth" width="400" />
<p class="caption">
Figure 5.1: Verhulst’s logistic function modeled both the exponential nature and the natural limit of population growth
</p>
</div>
<p>In the early 20th century, starting with applications in economics and in chemistry, the logistic function was adopted in a wide array of fields as a useful tool for modeling phenomena. In statistics, it was observed that the logistic function has a similar S-shape (or <em>sigmoid</em>) to a cumulative normal distribution of probability, as depicted in Figure <a href="bin-log-reg.html#fig:norm-log-curves">5.2</a><a href="#fn24" class="footnote-ref" id="fnref24"><sup>24</sup></a>, where the <span class="math inline">\(x\)</span> scale represents standard deviations around a mean. As we will learn, the logistic function gives rise to a mathematical model where the coefficients are easily interpreted in terms of likelihood of the outcome. Unsurprisingly, therefore, the logistic model soon became a common approach to modeling probabilistic phenomena.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:norm-log-curves"></span>
<img src="_main_files/figure-html/norm-log-curves-1.png" alt="The logistic function (blue dashed line) is very similar to a cumulative normal distribution (red solid line) but easier to interpret" width="672" />
<p class="caption">
Figure 5.2: The logistic function (blue dashed line) is very similar to a cumulative normal distribution (red solid line) but easier to interpret
</p>
</div>
</div>
<div id="use-cases-for-binomial-logistic-regression" class="section level3 hasAnchor" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> Use cases for binomial logistic regression<a href="bin-log-reg.html#use-cases-for-binomial-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Binomial logistic regression can be used when the outcome of interest is binary or dichotomous in nature. That is, it takes one of two values. For example, one or zero, true or false, yes or no. These classes are commonly described as ‘positive’ and ‘negative’ classes. There is an underlying assumption that the cumulative probability of the outcome takes a shape similar to a cumulative normal distribution.</p>
<p>Here are some example questions that could be approached using binomial logistic regression:</p>
<ul>
<li>Given a set of data about sales managers in an organization, including performance against targets, team size, tenure in the organization and other factors, what influence do these factors have on the likelihood of the individual receiving a high performance rating?</li>
<li>Given a set of demographic, income and location data, what influence does each have on the likelihood of an individual voting in an election?</li>
<li>Given a set of statistics about the in-game activity of soccer players, what relationship does each statistic have with the likelihood of a player scoring a goal?</li>
</ul>
</div>
<div id="walkthrough-logit" class="section level3 hasAnchor" number="5.1.3">
<h3><span class="header-section-number">5.1.3</span> Walkthrough example<a href="bin-log-reg.html#walkthrough-logit" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>You are an analyst for a large company consisting of regional sales teams across the country. Twice every year, this company promotes some of its salespeople. Promotion is at the discretion of the head of each regional sales team, taking into consideration financial performance, customer satisfaction ratings, recent performance ratings and personal judgment.</p>
<p>You are asked by the management of the company to conduct an analysis to determine how the factors of financial performance, customer ratings and performance ratings influence the likelihood of a given salesperson being promoted. You are provided with a data set containing data for the last three years of salespeople considered for promotion. The <code>salespeople</code> data set contains the following fields:</p>
<ul>
<li><code>promoted</code>: A binary value indicating 1 if the individual was promoted and 0 if not</li>
<li><code>sales</code>: the sales (in thousands of dollars) attributed to the individual in the period of the promotion</li>
<li><code>customer_rate</code>: the average satisfaction rating from a survey of the individual’s customers during the promotion period</li>
<li><code>performance</code>: the most recent performance rating prior to promotion, from 1 (lowest) to 4 (highest)</li>
</ul>
<p>Let’s take a quick look at the data.</p>
<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb292-1"><a href="bin-log-reg.html#cb292-1" tabindex="-1"></a><span class="co"># if needed, download salespeople data</span></span>
<span id="cb292-2"><a href="bin-log-reg.html#cb292-2" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="st">&quot;http://peopleanalytics-regression-book.org/data/salespeople.csv&quot;</span></span>
<span id="cb292-3"><a href="bin-log-reg.html#cb292-3" tabindex="-1"></a>salespeople <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(url)</span></code></pre></div>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb293-1"><a href="bin-log-reg.html#cb293-1" tabindex="-1"></a><span class="co"># look at the first few rows of data</span></span>
<span id="cb293-2"><a href="bin-log-reg.html#cb293-2" tabindex="-1"></a><span class="fu">head</span>(salespeople)</span></code></pre></div>
<pre><code>##   promoted sales customer_rate performance
## 1        0   594          3.94           2
## 2        0   446          4.06           3
## 3        1   674          3.83           4
## 4        0   525          3.62           2
## 5        1   657          4.40           3
## 6        1   918          4.54           2</code></pre>
<p>The data looks as expected. Let’s get a summary of the data.</p>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb295-1"><a href="bin-log-reg.html#cb295-1" tabindex="-1"></a><span class="fu">summary</span>(salespeople)</span></code></pre></div>
<pre><code>##     promoted          sales       customer_rate    performance 
##  Min.   :0.0000   Min.   :151.0   Min.   :1.000   Min.   :1.0  
##  1st Qu.:0.0000   1st Qu.:389.2   1st Qu.:3.000   1st Qu.:2.0  
##  Median :0.0000   Median :475.0   Median :3.620   Median :3.0  
##  Mean   :0.3219   Mean   :527.0   Mean   :3.608   Mean   :2.5  
##  3rd Qu.:1.0000   3rd Qu.:667.2   3rd Qu.:4.290   3rd Qu.:3.0  
##  Max.   :1.0000   Max.   :945.0   Max.   :5.000   Max.   :4.0  
##                   NA&#39;s   :1       NA&#39;s   :1       NA&#39;s   :1</code></pre>
<p>First we see a small number of missing values, and we should remove those observations. We see that about a third of individuals were promoted, that sales ranged from $151k to $945k, that as expected the average satisfaction ratings range from 1 to 5, and finally we see four performance ratings, although the performance categories are numeric when they should be an ordered factor, and <code>promoted</code> is numeric when it should be categorical. Let’s convert these, and then let’s do a pairplot to get a quick view on some possible underlying relationships, as in Figure <a href="bin-log-reg.html#fig:log-pairplot">5.3</a>.</p>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb297-1"><a href="bin-log-reg.html#cb297-1" tabindex="-1"></a><span class="fu">library</span>(GGally)</span>
<span id="cb297-2"><a href="bin-log-reg.html#cb297-2" tabindex="-1"></a></span>
<span id="cb297-3"><a href="bin-log-reg.html#cb297-3" tabindex="-1"></a><span class="co"># remove NAs</span></span>
<span id="cb297-4"><a href="bin-log-reg.html#cb297-4" tabindex="-1"></a>salespeople <span class="ot">&lt;-</span> salespeople[<span class="fu">complete.cases</span>(salespeople), ]</span>
<span id="cb297-5"><a href="bin-log-reg.html#cb297-5" tabindex="-1"></a></span>
<span id="cb297-6"><a href="bin-log-reg.html#cb297-6" tabindex="-1"></a><span class="co"># convert performance to ordered factor and promoted to categorical</span></span>
<span id="cb297-7"><a href="bin-log-reg.html#cb297-7" tabindex="-1"></a>salespeople<span class="sc">$</span>performance <span class="ot">&lt;-</span> <span class="fu">ordered</span>(salespeople<span class="sc">$</span>performance, </span>
<span id="cb297-8"><a href="bin-log-reg.html#cb297-8" tabindex="-1"></a>                                   <span class="at">levels =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)</span>
<span id="cb297-9"><a href="bin-log-reg.html#cb297-9" tabindex="-1"></a>salespeople<span class="sc">$</span>promoted <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(salespeople<span class="sc">$</span>promoted)</span>
<span id="cb297-10"><a href="bin-log-reg.html#cb297-10" tabindex="-1"></a></span>
<span id="cb297-11"><a href="bin-log-reg.html#cb297-11" tabindex="-1"></a><span class="co"># generate pairplot</span></span>
<span id="cb297-12"><a href="bin-log-reg.html#cb297-12" tabindex="-1"></a>GGally<span class="sc">::</span><span class="fu">ggpairs</span>(salespeople)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:log-pairplot"></span>
<img src="_main_files/figure-html/log-pairplot-1.png" alt="Pairplot for the `salespeople` data set" width="672" />
<p class="caption">
Figure 5.3: Pairplot for the <code>salespeople</code> data set
</p>
</div>
<p>We can see from this pairplot that there are clearly higher sales for those who are promoted versus those who are not. We also see a moderate relationship between customer rating and sales, which is intuitive (if the customer doesn’t think much of you, sales wouldn’t likely be very high).</p>
<p>So we can see that some relationships with our outcome may exist here, but it’s not clear how to tease them out and quantify them relative to each other. Let’s explore how binomial logistic regression can help us do this.</p>
</div>
</div>
<div id="mod-prob" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Modeling probabilistic outcomes using a logistic function<a href="bin-log-reg.html#mod-prob" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Imagine that you have an outcome event <span class="math inline">\(y\)</span> which either occurs or does not occur. The probability of <span class="math inline">\(y\)</span> occurring, or <span class="math inline">\(P(y = 1)\)</span>, obviously takes a value between 0 and 1. Now imagine that some input variable <span class="math inline">\(x\)</span> has a positive effect on the probability of the event occurring. Then you would naturally expect <span class="math inline">\(P(y = 1)\)</span> to increase as <span class="math inline">\(x\)</span> increases.</p>
<p>In our <code>salespeople</code> data set, let’s plot our <code>promotion</code> outcome against the <code>sales</code> input. This can be seen in Figure <a href="bin-log-reg.html#fig:prom-sales-plot">5.4</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:prom-sales-plot"></span>
<img src="_main_files/figure-html/prom-sales-plot-1.png" alt="Plot of promotion against sales in the `salespeople` data set" width="672" />
<p class="caption">
Figure 5.4: Plot of promotion against sales in the <code>salespeople</code> data set
</p>
</div>
<p>It’s clear that promotion is more likely with higher sales levels. As we move along the <span class="math inline">\(x\)</span> axis from left to right and gradually include more and more individuals with higher sales, we know that the probability of promotion is gradually increasing overall. We could try to model this probability using our logistic function, which we learned about in Section <a href="bin-log-reg.html#logistic-origins">5.1.1</a>. For example, let’s plot the logistic function
<span class="math display">\[
P(y = 1) = \frac{1}{1 + e^{-k(x - x_{0})}}
\]</span></p>
<p>on this data, where we set <span class="math inline">\(x_0\)</span> to the mean of <code>sales</code> and <span class="math inline">\(k\)</span> to be some maximum gradient value. In Figure <a href="bin-log-reg.html#fig:prom-with-logistic">5.5</a> we can see these logistic functions for different values of <span class="math inline">\(k\)</span>. All of these seem to reflect the pattern we are observing to some extent, but how do we determine the best-fitting logistic function?</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:prom-with-logistic"></span>
<img src="_main_files/figure-html/prom-with-logistic-1.png" alt="Overlaying logistic functions with various gradients onto previous plot" width="672" />
<p class="caption">
Figure 5.5: Overlaying logistic functions with various gradients onto previous plot
</p>
</div>
<div id="deriving-the-concept-of-log-odds" class="section level3 hasAnchor" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> Deriving the concept of log odds<a href="bin-log-reg.html#deriving-the-concept-of-log-odds" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s look more carefully at the index of the exponential constant <span class="math inline">\(e\)</span> in the denominator of our logistic function. Note that, because <span class="math inline">\(x_{0}\)</span> is a constant, we have:</p>
<p><span class="math display">\[
-k(x - x_{0}) = -(-kx_{0} + kx) = -(\beta_{0} + \beta_1x)
\]</span>
where <span class="math inline">\(\beta_0 = -kx_0\)</span> and <span class="math inline">\(\beta_{1} = k\)</span>. Therefore,</p>
<p><span class="math display">\[
P(y = 1) = \frac{1}{1 + e^{-(\beta_0 + \beta_1x)}}
\]</span></p>
<p>This equation makes intuitive sense. As the value of <span class="math inline">\(x\)</span> increases, the value <span class="math inline">\(e^{-(\beta_0 + \beta_1x)}\)</span> gets smaller and smaller towards zero, and thus <span class="math inline">\(P(y = 1)\)</span> approaches its theoretical maximum value of 1. As the value of <span class="math inline">\(x\)</span> decreases towards zero, we see that the value of <span class="math inline">\(P(y = 1)\)</span> approaches a minimum value of <span class="math inline">\(\frac{1}{1 + e^{-\beta_0}}\)</span>. Referring back to our salespeople example, we can thus see that <span class="math inline">\(\beta_0\)</span> helps determine the baseline probability of promotion assuming no sales at all. If <span class="math inline">\(\beta_0\)</span> has an extremely negative value, this baseline probability will approach its theoretical minimum of zero.</p>
<p>Let’s formalize the role of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> in the likelihood of a positive outcome. We know that for any binary event <span class="math inline">\(y\)</span>, <span class="math inline">\(P(y = 0)\)</span> is equal to <span class="math inline">\(1 - P(y = 1)\)</span>, so</p>
<p><span class="math display">\[
\begin{aligned}
P(y = 0) &amp;= 1 - \frac{1}{1 + e^{-(\beta_0 + \beta_1x)}} \\
&amp;= \frac{1 + e^{-(\beta_0 + \beta_1x)} - 1}{1 + e^{-(\beta_0 + \beta_1x)}} \\
&amp;= \frac{e^{-(\beta_0 + \beta_1x)}}{1 + e^{-(\beta_0 + \beta_1x)}}
\end{aligned}
\]</span></p>
<p>Putting these together, we find that</p>
<p><span class="math display">\[
\begin{aligned}
\frac{P(y = 1)}{P(y = 0)} &amp;= \frac{\frac{1}{1 + e^{-(\beta_0 + \beta_1x)}}}{\frac{e^{-(\beta_0 + \beta_1x)}}{1 + e^{-(\beta_0 + \beta_1x)}}} \\
&amp;= \frac{1}{e^{-(\beta_0 + \beta_1x)}} \\
&amp;= e^{\beta_0 + \beta_1x}
\end{aligned}
\]</span></p>
<p>or alternatively, if we apply the natural logarithm to both sides</p>
<p><span class="math display">\[
\ln\left(\frac{P(y = 1)}{P(y = 0)}\right) = \beta_0 + \beta_1x
\]</span></p>
<p>The right-hand side should look familiar from the previous chapter on linear regression, meaning there is something here we can model linearly. But what is the left-hand side?</p>
<p><span class="math inline">\(P(y = 1)\)</span> is the probability that the event will occur, while <span class="math inline">\(P(y = 0)\)</span> is the probability that the event will not occur. You may be familiar from sports like horse racing or other gambling situations that the ratio of these two represents the <em>odds</em> of an event. For example, if a given horse has odds of 1:4, this means that there is a 20% probability they will win and an 80% probability they will not<a href="#fn25" class="footnote-ref" id="fnref25"><sup>25</sup></a>.</p>
<p>Therefore we can conclude that the natural logarithm of the odds of <span class="math inline">\(y\)</span>—usually termed the <em>log odds</em> of <span class="math inline">\(y\)</span>—is linear in <span class="math inline">\(x\)</span>, and therefore we can model the log odds of <span class="math inline">\(y\)</span> using similar linear regression methods to those studied in Chapter <a href="linear-reg-ols.html#linear-reg-ols">4</a><a href="#fn26" class="footnote-ref" id="fnref26"><sup>26</sup></a>.</p>
</div>
<div id="modeling-the-log-odds-and-interpreting-the-coefficients" class="section level3 hasAnchor" number="5.2.2">
<h3><span class="header-section-number">5.2.2</span> Modeling the log odds and interpreting the coefficients<a href="bin-log-reg.html#modeling-the-log-odds-and-interpreting-the-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s take our simple case of regressing the <code>promoted</code> outcome against <code>sales</code>. We use a standard binomial GLM function and our standard formula notation which we learned in the previous chapter.</p>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="bin-log-reg.html#cb298-1" tabindex="-1"></a><span class="co"># run a binomial model </span></span>
<span id="cb298-2"><a href="bin-log-reg.html#cb298-2" tabindex="-1"></a>sales_model <span class="ot">&lt;-</span> <span class="fu">glm</span>(<span class="at">formula =</span> promoted <span class="sc">~</span> sales, </span>
<span id="cb298-3"><a href="bin-log-reg.html#cb298-3" tabindex="-1"></a>                   <span class="at">data =</span> salespeople, <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb298-4"><a href="bin-log-reg.html#cb298-4" tabindex="-1"></a></span>
<span id="cb298-5"><a href="bin-log-reg.html#cb298-5" tabindex="-1"></a><span class="co"># view the coefficients</span></span>
<span id="cb298-6"><a href="bin-log-reg.html#cb298-6" tabindex="-1"></a>sales_model<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>##  (Intercept)        sales 
## -21.77642020   0.03675848</code></pre>
<p>We can interpret the coefficients as follows:</p>
<ol style="list-style-type: decimal">
<li><p>The <code>(Intercept)</code> coefficient is the value of the log odds with zero input value of <span class="math inline">\(x\)</span>—it is the log odds of promotion if you made no sales.</p></li>
<li><p>The <code>sales</code> coefficient represents the increase in the log odds of promotion associated with each unit increase in sales.</p></li>
</ol>
<p>We can convert these coefficients from log odds to odds by applying the exponent function, to return to the identity we had previously</p>
<p><span class="math display">\[
\frac{P(y = 1)}{P(y = 0)} = e^{\beta_0 + \beta_1x} = e^{\beta_0}(e^{\beta_1})^x
\]</span></p>
<p>From this, we can interpret that <span class="math inline">\(e^{\beta_0}\)</span> represents the base odds of promotion assuming no sales, and that for every additional unit sales, those base odds are multiplied by <span class="math inline">\(e^{\beta_1}\)</span>. Given this multiplicative effect that <span class="math inline">\(e^{\beta_1}\)</span> has on the odds, it is known as an <em>odds ratio</em>.</p>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="bin-log-reg.html#cb300-1" tabindex="-1"></a><span class="co"># convert log odds to base odds and odds ratio</span></span>
<span id="cb300-2"><a href="bin-log-reg.html#cb300-2" tabindex="-1"></a><span class="fu">exp</span>(sales_model<span class="sc">$</span>coefficients)</span></code></pre></div>
<pre><code>##  (Intercept)        sales 
## 3.488357e-10 1.037442e+00</code></pre>
<p>So we can see that the base odds of promotion with zero sales is very close to zero, which makes sense. Note that odds can only be precisely zero in a situation where it is impossible to be in the positive class (that is, nobody gets promoted). We can also see that each unit (that is, every $1000) of sales multiplies the base odds by approximately 1.04—in other words, it increases the odds of promotion by 4%.</p>
</div>
<div id="odds-versus-probability" class="section level3 hasAnchor" number="5.2.3">
<h3><span class="header-section-number">5.2.3</span> Odds versus probability<a href="bin-log-reg.html#odds-versus-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>It is worth spending a little time understanding the concept of odds and how it relates to probability. It is extremely common for these two terms to be used synonymously, and this can lead to serious misunderstandings when interpreting a logistic regression model.</p>
<p>If a certain event has a probability of 0.1, then this means that its odds are 1:9, or 0.111. If the probability is 0.5, then the odds are 1, if the probability is 0.9, then the odds are 9, and if the probability is 0.99, the odds are 99. As we approach a probability of 1, the odds become exponentially large, as illustrated in Figure <a href="bin-log-reg.html#fig:odds-prob">5.6</a>:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:odds-prob"></span>
<img src="_main_files/figure-html/odds-prob-1.png" alt="Odds plotted against probability" width="672" />
<p class="caption">
Figure 5.6: Odds plotted against probability
</p>
</div>
<p>The consequence of this is that a given increase in odds can have a different effect on probability depending on what the original probability was in the first place. If the probability was already quite low, for example 0.1, then a 4% increase in odds translates to odds of 0.116, which translates to a new probability of 0.103586, representing an increase in probability of 3.59%, which is very close to the increase in odds. If the probability was already high, say 0.9, then a 4% increase in odds translates to odds of 9.36, which translates to a new probability of 0.903475 representing an increase in probability of 0.39%, which is very different from the increase in odds. Figure <a href="bin-log-reg.html#fig:pcoddsplot">5.7</a> shows the impact of a 4% increase in odds according to the original probability of the event.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pcoddsplot"></span>
<img src="_main_files/figure-html/pcoddsplot-1.png" alt="Effect of 4\% increase in odds plotted against original probability" width="672" />
<p class="caption">
Figure 5.7: Effect of 4% increase in odds plotted against original probability
</p>
</div>
<p>We can see that the closer the base probability is to zero, the similar the effect of the increase on both odds and on probability. However, the higher the probability of the event, the less impact the increase in odds has. In any case, it’s useful to remember the formulas for converting odds to probability and vice versa. If <span class="math inline">\(O\)</span> represents odds and <span class="math inline">\(P\)</span> represents probability then we have:</p>
<p><span class="math display">\[
\begin{aligned}
O &amp;= \frac{P}{1 - P} \\
P &amp;= \frac{O}{1 + O}
\end{aligned}
\]</span></p>
</div>
</div>
<div id="running-a-multivariate-binomial-logistic-regression-model" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> Running a multivariate binomial logistic regression model<a href="bin-log-reg.html#running-a-multivariate-binomial-logistic-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The derivations in the previous section extend to multivariate data. Let <span class="math inline">\(y\)</span> be a dichotomous outcome, and let <span class="math inline">\(x_1, x_2, \dots, x_p\)</span> be our input variables. Then</p>
<p><span class="math display">\[
\ln\left(\frac{P(y = 1)}{P(y = 0)}\right) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_px_p
\]</span>
for coefficients <span class="math inline">\(\beta_0, \beta_1,\dots, \beta_p\)</span>. As before:</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> represents the log odds of our outcome when all inputs are zero</li>
<li>Each <span class="math inline">\(\beta_i\)</span> represents the increase in the log odds of our outcome associated with a unit change in <span class="math inline">\(x_i\)</span>, assuming no change in other inputs.</li>
</ul>
<p>Applying an exponent as before, we have</p>
<p><span class="math display">\[
\begin{aligned}
\frac{P(y = 1)}{P(y = 0)} &amp;= e^{\beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_px_p} \\
&amp;= e^{\beta_0}(e^{\beta_1})^{x_1}(e^{\beta_2})^{x_2}\dots(e^{\beta_p})^{x_p}
\end{aligned}
\]</span></p>
<p>Therefore we can conclude that:</p>
<ul>
<li><span class="math inline">\(e^{\beta_0}\)</span> represents the odds of the outcome when all inputs are zero.</li>
<li>Each <span class="math inline">\(e^{\beta_i}\)</span> represents the <em>odds ratio</em> associated with a unit increase in <span class="math inline">\(x_i\)</span> assuming no change in the other inputs (that is, a unit increase in <span class="math inline">\(x_i\)</span> multiplies the odds of our outcome by <span class="math inline">\(e^{\beta_i}\)</span>).</li>
</ul>
<p>Let’s put this into practice.</p>
<div id="running-and-interpreting-a-multivariate-binomial-logistic-regression-model" class="section level3 hasAnchor" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Running and interpreting a multivariate binomial logistic regression model<a href="bin-log-reg.html#running-and-interpreting-a-multivariate-binomial-logistic-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s use a binomial logistic regression model to understand how each of the three inputs in our <code>salespeople</code> data set influence the likelihood of promotion.</p>
<p>First, as we learned previously, it is good practice to convert the categorical <code>performance</code> variable to a dummy variable<a href="#fn27" class="footnote-ref" id="fnref27"><sup>27</sup></a>.</p>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="bin-log-reg.html#cb302-1" tabindex="-1"></a><span class="fu">library</span>(makedummies)</span>
<span id="cb302-2"><a href="bin-log-reg.html#cb302-2" tabindex="-1"></a></span>
<span id="cb302-3"><a href="bin-log-reg.html#cb302-3" tabindex="-1"></a><span class="co"># convert performance to dummy</span></span>
<span id="cb302-4"><a href="bin-log-reg.html#cb302-4" tabindex="-1"></a>salespeople_dummies <span class="ot">&lt;-</span> makedummies<span class="sc">::</span><span class="fu">makedummies</span>(salespeople)</span>
<span id="cb302-5"><a href="bin-log-reg.html#cb302-5" tabindex="-1"></a></span>
<span id="cb302-6"><a href="bin-log-reg.html#cb302-6" tabindex="-1"></a><span class="co"># check it worked</span></span>
<span id="cb302-7"><a href="bin-log-reg.html#cb302-7" tabindex="-1"></a><span class="fu">head</span>(salespeople_dummies)</span></code></pre></div>
<pre><code>##   promoted sales customer_rate performance_2 performance_3 performance_4
## 1        0   594          3.94             1             0             0
## 2        0   446          4.06             0             1             0
## 3        1   674          3.83             0             0             1
## 4        0   525          3.62             1             0             0
## 5        1   657          4.40             0             1             0
## 6        1   918          4.54             1             0             0</code></pre>
<p>Now we can run our model (using the formula <code>promoted ~ .</code> to mean regressing <code>promoted</code> against everything else) and view our coefficients.</p>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="bin-log-reg.html#cb304-1" tabindex="-1"></a><span class="co"># run binomial glm</span></span>
<span id="cb304-2"><a href="bin-log-reg.html#cb304-2" tabindex="-1"></a>full_model <span class="ot">&lt;-</span> <span class="fu">glm</span>(<span class="at">formula =</span> <span class="st">&quot;promoted ~ .&quot;</span>,</span>
<span id="cb304-3"><a href="bin-log-reg.html#cb304-3" tabindex="-1"></a>                  <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>,</span>
<span id="cb304-4"><a href="bin-log-reg.html#cb304-4" tabindex="-1"></a>                  <span class="at">data =</span> salespeople_dummies)</span>
<span id="cb304-5"><a href="bin-log-reg.html#cb304-5" tabindex="-1"></a></span>
<span id="cb304-6"><a href="bin-log-reg.html#cb304-6" tabindex="-1"></a><span class="co"># get coefficient summary </span></span>
<span id="cb304-7"><a href="bin-log-reg.html#cb304-7" tabindex="-1"></a>(coefs <span class="ot">&lt;-</span> <span class="fu">summary</span>(full_model)<span class="sc">$</span>coefficients)</span></code></pre></div>
<pre><code>##                   Estimate  Std. Error    z value     Pr(&gt;|z|)
## (Intercept)   -19.85893195 3.444078811 -5.7661085 8.112287e-09
## sales           0.04012425 0.006576429  6.1012212 1.052611e-09
## customer_rate  -1.11213130 0.482681585 -2.3040682 2.121881e-02
## performance_2   0.26299953 1.021980179  0.2573431 7.969139e-01
## performance_3   0.68495453 0.982166998  0.6973911 4.855581e-01
## performance_4   0.73449340 1.071963758  0.6851849 4.932272e-01</code></pre>
<p>Note how only three of the <code>performance</code> dummies have displayed. This is because everyone is in one of the four performance categories, so the model is using <code>performance_1</code> as the reference case. We can interpret each performance coefficient as the effect of a move to that performance category from <code>performance_1</code>.</p>
<p>We can already see from the last column of our coefficient summary—the coefficient p-values—that only <code>sales</code> and <code>customer_rate</code> meet the significance threshold of less than 0.05. Interestingly, it appears from the <code>Estimate</code> column that <code>customer_rate</code> has a negative effect on the log odds of promotion. For convenience, we can add an extra column to our coefficient summary to create the exponents of our estimated coefficients so that we can see the odds ratios. We can also remove columns that are less useful to us if we wish.</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="bin-log-reg.html#cb306-1" tabindex="-1"></a><span class="co"># create coefficient table with estimates, p-values and odds ratios</span></span>
<span id="cb306-2"><a href="bin-log-reg.html#cb306-2" tabindex="-1"></a>(full_coefs <span class="ot">&lt;-</span> <span class="fu">cbind</span>(coefs[ ,<span class="fu">c</span>(<span class="st">&quot;Estimate&quot;</span>, <span class="st">&quot;Pr(&gt;|z|)&quot;</span>)], </span>
<span id="cb306-3"><a href="bin-log-reg.html#cb306-3" tabindex="-1"></a>                     <span class="at">odds_ratio =</span> <span class="fu">exp</span>(full_model<span class="sc">$</span>coefficients))) </span></code></pre></div>
<pre><code>##                   Estimate     Pr(&gt;|z|)   odds_ratio
## (Intercept)   -19.85893195 8.112287e-09 2.373425e-09
## sales           0.04012425 1.052611e-09 1.040940e+00
## customer_rate  -1.11213130 2.121881e-02 3.288573e-01
## performance_2   0.26299953 7.969139e-01 1.300826e+00
## performance_3   0.68495453 4.855581e-01 1.983682e+00
## performance_4   0.73449340 4.932272e-01 2.084426e+00</code></pre>
<p>Now we can interpret our model as follows:</p>
<ul>
<li>All else being equal, sales have a significant positive effect on the likelihood of promotion, with each additional thousand dollars of sales increasing the odds of promotion by 4%</li>
<li>All else being equal, customer ratings have a significant negative effect on the likelihood of promotion, with one full rating higher associated with 67% lower odds of promotion</li>
<li>All else being equal, performance ratings have no significant effect on the likelihood of promotion</li>
</ul>
<p>The second conclusion may appear counter-intuitive, but remember from our pairplot in Section <a href="bin-log-reg.html#walkthrough-logit">5.1.3</a> that there is already moderate correlation between sales and customer ratings, and this model will be controlling for that relationship. Recall that our odds ratios act <em>assuming all other variables are the same</em>. Therefore, if two individuals have the same sales and performance ratings, the one with the lower customer rating is more likely to have been promoted. Similarly, if two individuals have the same level of sales and the same customer rating, their performance rating will have no significant bearing on the likelihood of promotion.</p>
<p>Many analysts will feel uncomfortable with stating these conclusions with too much precision, and therefore exponent confidence intervals can be calculated to provide a range for the odds ratios.</p>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="bin-log-reg.html#cb308-1" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">confint</span>(full_model))</span></code></pre></div>
<pre><code>##                      2.5 %       97.5 %
## (Intercept)   7.879943e-13 7.385387e-07
## sales         1.029762e+00 1.057214e+00
## customer_rate 1.141645e-01 7.793018e-01
## performance_2 1.800447e-01 1.061602e+01
## performance_3 3.060299e-01 1.547188e+01
## performance_4 2.614852e-01 1.870827e+01</code></pre>
<p>Therefore we can say that—all else being equal—every additional unit of sales increases the odds of promotion by between 3.0% and 5.7%, and every additional point in customer rating decreases the odds of promotion by between 22% and 89%.</p>
<p>Similar to other regression models, the unit scale needs to be taken into consideration during interpretation. On first sight, a decrease of up to 89% in odds seems a lot more important than an increase of up to 5.7% in odds. However, the increase of up to 5.7% is for one unit ($1000) in many thousands of sales units, and over 10 or 100 additional units can have a substantial compound effect on odds of promotion. The decrease of up to 89% is on a full customer rating point on a scale of only 4 full points.</p>
</div>
<div id="logistic-gof" class="section level3 hasAnchor" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Understanding the fit and goodness-of-fit of a binomial logistic regression model<a href="bin-log-reg.html#logistic-gof" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Understanding the fit of a binomial logistic regression model is not straightforward and is sometimes controversial. Before we discuss this, let’s simplify our model based on our learning that the performance data has no significant effect on the outcome.</p>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="bin-log-reg.html#cb310-1" tabindex="-1"></a><span class="co"># simplify model</span></span>
<span id="cb310-2"><a href="bin-log-reg.html#cb310-2" tabindex="-1"></a>simpler_model <span class="ot">&lt;-</span> <span class="fu">glm</span>(<span class="at">formula =</span> promoted <span class="sc">~</span> sales <span class="sc">+</span> customer_rate,</span>
<span id="cb310-3"><a href="bin-log-reg.html#cb310-3" tabindex="-1"></a>                     <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>,</span>
<span id="cb310-4"><a href="bin-log-reg.html#cb310-4" tabindex="-1"></a>                     <span class="at">data =</span> salespeople)</span></code></pre></div>
<p>As in the previous chapter, again we have the luxury of a three-dimensional model, so we can visualize it in Interactive Figure <a href="bin-log-reg.html#fig:log-reg-3d">5.8</a>, revealing a 3D sigmoid curve which ‘twists’ to reflect the relative influence of <code>sales</code> and <code>customer_rate</code> on the outcome.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:log-reg-3d"></span>
<div class="plotly html-widget html-fill-item" id="htmlwidget-4c52b7f36e26c300a71d" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-4c52b7f36e26c300a71d">{"x":{"visdat":{"6f1952150094":["function () ","plotlyVisDat"]},"cur_data":"6f1952150094","attrs":{"6f1952150094":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":{},"y":{},"z":{},"mode":"markers","type":"scatter3d","marker":{"size":5,"color":"blue","symbol":104},"name":"Observations","inherit":true},"6f1952150094.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":[0.51304378747035551,0.0023290366332890692,0.96791325675030404,0.085055170809732725,0.88899508542332528,0.99999614001934445,3.9415846469361267e-05,3.352637061940568e-05,5.0106017726651335e-05,0.00070718824783279506,0.27697229377768701,0.99714208922820258,0.2769377822903164,0.007070981214073107,0.0001218429311525499,0.00014545030098593996,6.8491527318655645e-06,0.00014321157953073481,0.0015289282925671989,0.99999699754650007,0.99241673066241309,0.0048063312419649052,0.050910564453652336,5.8628538706006156e-05,0.99980982241345651,0.99694331729095731,0.0001186154653762814,2.2450005899479074e-06,2.1214280004874962e-05,0.99775580694991217,0.016036248583680107,0.017208780600967592,3.2517296124473693e-06,0.99995799273909569,0.0006596449954445738,0.99899372927826502,0.99800632442776227,0.0016350908703035212,0.99999879815048631,0.14282719379846778,0.00052361457648373352,1.0280406943761922e-05,0.9999989978988828,0.055555992634509367,7.9557243984769329e-05,0.00078402305155005731,0.015720918156021394,0.0012331624585504601,0.0091551843473660962,0.96463508618113314,0.29899149817866943,0.0017216911187149413,0.0073114941555897485,0.0029926706262927757,8.1348627735225868e-05,0.00099196858908719458,0.99984904633485516,0.00016940471150803441,0.99881729861370971,0.00087874561177734461,0.99940564349091898,0.0019427004013640246,0.99495641215681196,0.96940927140424216,0.00098304502670206405,0.044122862433741278,0.036672143743222155,0.011895056870476059,0.097308774617811386,5.9320821088611871e-06,5.1821303396215802e-05,0.00012268280264469275,0.1303804896447627,0.001219489670595294,0.086098936085858738,0.65425753186011137,0.99981109171099136,0.0020825258785461833,0.00057400940535779637,0.033912541626998936,0.99974082925032526,0.18841996879761905,0.0055577353214503094,0.98533826894573695,0.91573089402718044,0.00085938512230807485,0.0039938342606073377,0.99911826782934265,3.9173467464395717e-06,0.0010972604175767009,0.99948294128294379,0.63263064701270433,0.99999875165777696,0.19020373819174932,0.00022284075695328337,0.0018085228949677175,0.032682631919285034,0.0015050822414446438,0.87598158130773651,0.00066712164962268606,0.00038420744661276621,0.00086723691551420293,0.00026654467279782761,0.0044752567006350651,0.023412393006145782,0.9006288609575599,0.0008200782825389276,0.004699636113897703,5.2529178436693641e-05,0.00045658288387549279,0.99830783831499514,0.76223049790283615,0.00058311244522132237,0.014266535715781948,0.00051651721979582149,0.00042785910127370248,0.00036655278360342165,0.0096102989524475494,0.99999773695658756,0.99733307281658812,0.23508414554954515,0.99648077593570084,0.22953554089935746,0.0017602922467597393,0.010003850485586389,0.0014389537746719299,0.99895925582698186,0.99998765784876553,0.0057462539301269579,0.99999932639894273,6.3450182926457407e-06,0.0015423128093645722,0.00032619079841053286,0.0062146454450792204,3.9059811993662365e-05,0.99999674495314139,5.7389157505199264e-07,1.5528951992312321e-05,0.99967637866056114,0.0028810388182020615,0.00018369090980264609,0.93634930744356715,0.006186598671841694,0.99999902695747767,0.0011019758107579944,0.0001587718911722378,0.00029161240487343476,0.023774400373556132,0.0044547698399390843,0.001372623539863253,0.0090904836788511106,6.3601135172150482e-06,0.99973845989494048,2.4552821804945914e-05,0.021732651155699299,0.99982182024230593,0.99984354833540134,0.99744436066869846,8.9989444663975531e-05,3.7778185556917635e-06,0.99997553514464321,0.00067783650979909803,0.00041191882277806995,0.0050496785530249272,0.99999933692064391,0.012546232690983443,0.014103604670684469,0.00043470779617807518,0.99999932959264359,0.99835930918194682,0.0025582934759558634,0.12709933152061567,0.99676094350262967,0.9833337325224083,6.9235778881309462e-05,0.93278095717935394,0.048899492572318445,0.91572646085628684,0.26231270573386523,0.012079812579912249,0.00033142215555053431,0.40915925473353232,0.87256176682337949,0.99934408759309046,0.0011974120550461449,0.00079629398434483752,0.89118761320262452,0.94415399972260761,0.99609126241652002,6.69325770237538e-05,0.00082911004754191624,0.002881368897839996,0.0017846030462786338,0.00076503923079844591,2.87341679526324e-06,3.8965990441351659e-05,0.95823462386235581,0.5040658355929899,0.99973967717072032,0.9999722038690072,0.0010492121203519843,0.97159380299331155,0.94806378084103504,0.0082988346485232505,0.0042790652228158034,0.34092411529972177,0.00012625904134048278,0.00028440922633490178,0.99997909217582026,0.00047980611565371056,0.0018004798476501307,0.99999582677410637,0.74093313103187652,0.030953852350013321,0.014746076810555455,0.00033357206718350553,0.0096545400760161808,0.99535644062420692,0.99251922871018583,0.96126841290298304,0.013111641004797222,9.9223647506071624e-07,0.99999727434573704,0.0012331270819546417,0.0010901312531541277,0.022118752212858351,3.5870396105639534e-05,0.99947358263883412,0.001324899622030971,0.99999976957577241,0.99915314059995608,0.0074253806275745996,0.00049504690665614129,0.03997419203936723,0.028009469676190751,0.045849207277916228,0.090815582137629816,0.99713530950094931,0.0087740321902465244,0.99999968380429438,0.99689496461488691,0.00028836783398084628,0.001772355220526106,0.45706390398532365,0.57859012934015497,0.0014554592433055141,0.0026457962736066861,0.99553943054857719,0.0018202893948287477,0.00059112433197930846,0.00036250592960833638,3.0790540845672001e-05,0.99999806228423704,8.9640057315764726e-06,0.99999787981238353,0.9996412130899206,0.99991692048426917,0.00081626354708174938,0.013552896352097932,6.4886446899741995e-06,0.00026369249809228101,1.1130584183612732e-07,0.0042608129416332914,0.0011502334142987,1.8262585398574964e-06,0.98878648267574631,0.86614512691255519,7.01553181649738e-05,0.0012606073776191568,3.0315699065442381e-05,0.27242678089869948,0.17739435943512302,0.99998355251200965,0.00028440922633490178,0.99303164717284531,5.0924160768164907e-06,0.00019389483785481831,0.029000194421931892,0.99985502079134347,0.00051669519286773877,0.99995516374662963,3.7181286730171773e-05,0.12786671084907283,0.0027001263816906467,0.00061540202446684123,0.10914417668719828,0.00019825454756834999,0.026754892253535365,1.3189731408195604e-05,0.99524180883524671,0.88428616618545175,0.99995683477809993,0.29803239897585726,0.026984649886865223,0.0019003792818226739,0.0024851474680908747,0.0041385990217830569,6.1328525300673314e-05,0.99994622856928528,2.8922196085978874e-05,0.045077852937048891,0.00012265814096523993,0.99998359201971809,8.0105660752544238e-05,0.011034808635005571,0.99828481113350187,0.99939895815811008,2.992188879146047e-06,0.42719484557893322,0.0067477527548032781,0.26101869185272308,0.003602848302585521,0.99999833979515551,0.00056503217303786706,5.8394230014411791e-06,0.013174026557107322,0.19017719453630735,0.00095505938651263274,0.18983358399887262,0.085941470536854969,0.99762640497296839,0.018464395284543383,0.0011869830472102157,0.97502954110509554,0.0079743174081624481,2.0341874859758787e-06,0.0024304808904847901,6.7529079488795032e-05,0.99972156085602804,0.99957164311569224,0.015787484366577113,0.99770963899072651,0.20326334646760974,7.1302115311237773e-06,0.0024201638965907863,0.9998407430005416,1.8389014027178741e-06,0.023407796251840553,0.00020589866357521993,2.4214986433855513e-05,0.0053733720102236036,0.99999734016410868,0.37231494492210748,0.9991081226482893,0.77655885885668718,0.015618494795570865,0.5802433622197114,0.0059572199248790403,0.017943635993395358,0.00067319868260557727],"x":{},"y":{},"type":"mesh3d","name":"Fitted values","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"sales"},"yaxis":{"title":"customer_rate","nticks":5},"camera":{"eye":{"x":-0.5,"y":2,"z":0}},"zaxis":{"title":"promoted"},"aspectmode":"cube"},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[594,446,674,525,657,918,318,364,342,387,527,716,557,450,344,372,258,338,410,937,702,469,535,342,819,736,330,274,341,717,478,487,239,825,400,728,773,425,943,510,389,270,945,497,329,389,475,383,432,619,578,411,445,440,359,419,840,393,754,441,803,444,753,688,431,511,464,473,532,280,342,320,531,373,547,611,825,431,401,517,803,586,444,693,659,416,423,756,245,419,757,617,909,516,317,425,528,416,645,390,393,394,387,450,487,607,369,489,324,417,694,651,395,442,422,404,381,501,944,753,591,735,538,451,477,436,738,902,464,944,285,453,382,414,335,935,203,348,800,436,360,674,425,901,453,350,362,486,471,459,506,262,825,291,464,802,818,736,364,308,862,349,375,423,938,456,517,373,898,777,470,545,699,697,300,677,497,669,596,492,346,590,592,780,432,418,662,678,716,330,414,416,403,362,284,363,655,597,794,818,409,681,606,489,475,590,396,420,857,371,421,828,594,533,462,392,475,752,659,650,496,211,898,388,383,455,319,756,377,940,757,469,394,484,491,547,519,739,479,943,742,357,432,584,595,401,460,753,466,362,361,338,882,293,922,793,787,400,516,295,307,151,441,406,270,680,662,347,453,309,592,540,886,420,718,284,323,513,841,362,842,321,516,428,383,521,358,489,252,720,610,871,594,522,379,454,450,317,835,297,516,355,858,305,410,707,798,265,576,448,590,456,930,412,286,440,546,385,544,505,732,506,394,674,458,251,429,348,789,795,509,754,580,289,390,787,241,522,412,359,489,940,592,796,653,459,586,401,500,373],"y":[3.9399999999999999,4.0599999999999996,3.8300000000000001,3.6200000000000001,4.4000000000000004,4.54,3.0899999999999999,4.8899999999999997,3.7400000000000002,3,2.4300000000000002,3.1600000000000001,3.5099999999999998,3.21,3.02,3.8700000000000001,2.4900000000000002,2.6600000000000001,3.1400000000000001,5,3.5299999999999998,4.2400000000000002,4.4699999999999998,3.6000000000000001,4.4500000000000002,3.9399999999999999,2.54,4.0599999999999996,4.4699999999999998,2.98,3.48,3.7400000000000002,2.4700000000000002,3.3199999999999998,3.5299999999999998,2.6600000000000001,4.8899999999999997,3.6200000000000001,4.4000000000000004,2.5600000000000001,3.3399999999999999,2.5600000000000001,4.3099999999999996,3.02,2.8599999999999999,2.98,3.3900000000000001,2.3599999999999999,2.3300000000000001,1.9399999999999999,4.1699999999999999,3.0699999999999998,3,3.6200000000000001,3.9199999999999999,3.8500000000000001,5,4.4900000000000002,3.7400000000000002,4.75,4.8899999999999997,4.1500000000000004,5,4.29,4.29,3.7400000000000002,2.2200000000000002,3.5699999999999998,3.7400000000000002,3.4100000000000001,3.71,2.1499999999999999,3.4100000000000001,2.0099999999999998,4.4000000000000004,4.0300000000000002,4.6600000000000001,3.6200000000000001,3.6899999999999999,4.2000000000000002,4.1500000000000004,5,3.21,3.7999999999999998,4.2000000000000002,3.8700000000000001,2.75,3.5499999999999998,2.52,3.7599999999999998,3.1099999999999999,4.3300000000000001,3.21,2.4700000000000002,1.51,3.5299999999999998,4.6299999999999999,3.3700000000000001,4.0800000000000001,3.1600000000000001,3.7599999999999998,3.0699999999999998,3.8700000000000001,3.6200000000000001,3.46,2.4900000000000002,2.2200000000000002,4.9800000000000004,3.0499999999999998,4.4699999999999998,1.8999999999999999,5,3.46,2.29,4.54,4.0599999999999996,3.3700000000000001,4.7699999999999996,5,4.4299999999999997,4.9299999999999997,4.0300000000000002,3.0499999999999998,4.4900000000000002,3.8700000000000001,4.1299999999999999,3.0499999999999998,5,3.8999999999999999,3.9199999999999999,3.5299999999999998,4.6799999999999997,3.5099999999999998,2.0299999999999998,3.71,5,2.7200000000000002,5,4.2400000000000002,3.5099999999999998,3.23,4.4699999999999998,2.4300000000000002,2.7000000000000002,4.9800000000000004,3,2.8900000000000001,3.4100000000000001,4.3799999999999999,5,5,2.7000000000000002,4.9500000000000002,2.54,2.7000000000000002,3.7799999999999998,4.2400000000000002,3.7799999999999998,4.0099999999999998,4.8200000000000003,4.1699999999999999,1.6699999999999999,3.0499999999999998,2.54,3.6899999999999999,2.9100000000000001,5,2.9300000000000002,2.2599999999999998,4.8600000000000003,4.8399999999999999,3.9399999999999999,2.6600000000000001,4.0599999999999996,1.9399999999999999,4.6299999999999999,3.1400000000000001,4.5599999999999996,4.9800000000000004,4.2400000000000002,2.2000000000000002,4.1699999999999999,2.2000000000000002,4.1500000000000004,4.1500000000000004,4.0099999999999998,4.5599999999999996,4.4900000000000002,3.4399999999999999,3.0499999999999998,3.8300000000000001,2.79,2.75,2.0299999999999998,4.2000000000000002,4.7199999999999998,3.3900000000000001,4.0800000000000001,3.8300000000000001,2.7000000000000002,3.4399999999999999,3.9700000000000002,1.8300000000000001,4.4699999999999998,4.5599999999999996,4.4299999999999997,4.8600000000000003,5,3.8500000000000001,2.77,3.3900000000000001,1.3700000000000001,3.0499999999999998,4.8600000000000003,2.98,3.8500000000000001,3.8300000000000001,4.8899999999999997,1.97,3.1400000000000001,4.3099999999999996,2.52,3.5099999999999998,2.54,2.4700000000000002,2.3599999999999999,3.21,3.0899999999999999,2.0800000000000001,2.8199999999999998,3.5499999999999998,3.8500000000000001,3.5699999999999998,2.8599999999999999,3.4399999999999999,5,3.3399999999999999,3.9900000000000002,4.0599999999999996,3.21,4.1699999999999999,2.7200000000000002,3.7999999999999998,3.7799999999999998,3.7400000000000002,2.8599999999999999,4.4500000000000002,4.8899999999999997,5,2.2599999999999998,2.6600000000000001,4.0300000000000002,2.6299999999999999,3.5099999999999998,4.1500000000000004,4.0800000000000001,2.5600000000000001,3.3399999999999999,5,3.8700000000000001,1,2.3100000000000001,3.3399999999999999,3.25,4.0999999999999996,3.0899999999999999,4.7699999999999996,3.6200000000000001,4.8600000000000003,3,4.79,3.4100000000000001,4.6799999999999997,5,4.0300000000000002,3.6899999999999999,1.8500000000000001,4.2000000000000002,5,2.3799999999999999,3.9900000000000002,3.25,2.8900000000000001,3.2799999999999998,2.98,3.23,3.0899999999999999,3.4100000000000001,1.6899999999999999,3.7599999999999998,2.75,5,4.75,4.5899999999999999,1.8300000000000001,4.29,3.6899999999999999,2.6600000000000001,3.8999999999999999,2.6099999999999999,3.8999999999999999,3.4100000000000001,3.6699999999999999,1.99,1.3700000000000001,2.3799999999999999,4.7199999999999998,3.48,3.6000000000000001,3.1800000000000002,4.7699999999999996,4.0300000000000002,4.2199999999999998,4.0999999999999996,3.6400000000000001,2.29,3.5499999999999998,2.6600000000000001,3.48,2.8900000000000001,3.5699999999999998,4.3600000000000003,2.79,3.6000000000000001,3.3900000000000001,3.3199999999999998,3.4100000000000001,3.6899999999999999,3.71,4.3099999999999996,4.6100000000000003,4.3300000000000001,4.7000000000000002,3.5699999999999998,2.0099999999999998,3.1400000000000001,3.0499999999999998,4.7199999999999998,5,5,4.8600000000000003,5,4.3799999999999999,5,5,2.8199999999999998,3.4100000000000001,1.6000000000000001,4.1699999999999999,2.54],"z":["0","0","1","0","1","1","0","0","0","0","0","1","0","0","0","0","0","0","0","1","1","0","0","0","1","1","0","0","0","1","0","0","0","1","0","1","1","0","1","0","0","0","1","0","0","0","0","0","1","1","1","0","0","0","0","0","1","0","1","0","1","0","1","1","0","0","0","0","0","0","0","0","0","0","0","1","1","0","0","0","1","0","0","1","1","0","0","1","0","0","1","1","1","0","0","0","0","0","1","0","0","0","0","0","0","1","0","0","0","0","1","1","0","0","0","0","0","0","1","1","0","1","1","0","0","0","1","1","0","1","0","0","0","0","0","1","0","0","1","0","0","1","0","1","0","0","0","0","0","0","0","0","1","0","1","1","1","1","0","0","1","0","0","0","1","0","0","0","1","1","0","0","1","1","0","1","0","1","1","0","0","1","0","1","0","0","1","1","1","0","0","0","0","0","0","0","1","0","1","1","0","1","1","0","0","0","0","0","1","0","0","1","0","0","0","0","0","1","1","1","0","0","1","0","0","0","0","1","0","1","1","0","0","0","0","0","0","1","0","1","1","0","0","0","1","0","0","1","0","0","0","0","1","0","1","1","1","0","0","0","0","0","0","0","0","1","1","0","0","0","0","0","1","0","1","0","0","0","1","0","1","0","0","0","0","1","0","0","0","1","1","1","0","0","0","0","0","0","1","0","0","0","1","0","0","1","1","0","1","0","0","0","1","0","0","0","0","0","0","0","1","0","0","1","0","0","0","0","1","1","0","1","0","0","0","1","0","0","0","0","0","1","0","1","1","0","0","0","0","0"],"mode":"markers","type":"scatter3d","marker":{"color":"blue","size":5,"symbol":104,"line":{"color":"rgba(31,119,180,1)"}},"name":"Observations","error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"promoted","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"z":[0.51304378747035551,0.0023290366332890692,0.96791325675030404,0.085055170809732725,0.88899508542332528,0.99999614001934445,3.9415846469361267e-05,3.352637061940568e-05,5.0106017726651335e-05,0.00070718824783279506,0.27697229377768701,0.99714208922820258,0.2769377822903164,0.007070981214073107,0.0001218429311525499,0.00014545030098593996,6.8491527318655645e-06,0.00014321157953073481,0.0015289282925671989,0.99999699754650007,0.99241673066241309,0.0048063312419649052,0.050910564453652336,5.8628538706006156e-05,0.99980982241345651,0.99694331729095731,0.0001186154653762814,2.2450005899479074e-06,2.1214280004874962e-05,0.99775580694991217,0.016036248583680107,0.017208780600967592,3.2517296124473693e-06,0.99995799273909569,0.0006596449954445738,0.99899372927826502,0.99800632442776227,0.0016350908703035212,0.99999879815048631,0.14282719379846778,0.00052361457648373352,1.0280406943761922e-05,0.9999989978988828,0.055555992634509367,7.9557243984769329e-05,0.00078402305155005731,0.015720918156021394,0.0012331624585504601,0.0091551843473660962,0.96463508618113314,0.29899149817866943,0.0017216911187149413,0.0073114941555897485,0.0029926706262927757,8.1348627735225868e-05,0.00099196858908719458,0.99984904633485516,0.00016940471150803441,0.99881729861370971,0.00087874561177734461,0.99940564349091898,0.0019427004013640246,0.99495641215681196,0.96940927140424216,0.00098304502670206405,0.044122862433741278,0.036672143743222155,0.011895056870476059,0.097308774617811386,5.9320821088611871e-06,5.1821303396215802e-05,0.00012268280264469275,0.1303804896447627,0.001219489670595294,0.086098936085858738,0.65425753186011137,0.99981109171099136,0.0020825258785461833,0.00057400940535779637,0.033912541626998936,0.99974082925032526,0.18841996879761905,0.0055577353214503094,0.98533826894573695,0.91573089402718044,0.00085938512230807485,0.0039938342606073377,0.99911826782934265,3.9173467464395717e-06,0.0010972604175767009,0.99948294128294379,0.63263064701270433,0.99999875165777696,0.19020373819174932,0.00022284075695328337,0.0018085228949677175,0.032682631919285034,0.0015050822414446438,0.87598158130773651,0.00066712164962268606,0.00038420744661276621,0.00086723691551420293,0.00026654467279782761,0.0044752567006350651,0.023412393006145782,0.9006288609575599,0.0008200782825389276,0.004699636113897703,5.2529178436693641e-05,0.00045658288387549279,0.99830783831499514,0.76223049790283615,0.00058311244522132237,0.014266535715781948,0.00051651721979582149,0.00042785910127370248,0.00036655278360342165,0.0096102989524475494,0.99999773695658756,0.99733307281658812,0.23508414554954515,0.99648077593570084,0.22953554089935746,0.0017602922467597393,0.010003850485586389,0.0014389537746719299,0.99895925582698186,0.99998765784876553,0.0057462539301269579,0.99999932639894273,6.3450182926457407e-06,0.0015423128093645722,0.00032619079841053286,0.0062146454450792204,3.9059811993662365e-05,0.99999674495314139,5.7389157505199264e-07,1.5528951992312321e-05,0.99967637866056114,0.0028810388182020615,0.00018369090980264609,0.93634930744356715,0.006186598671841694,0.99999902695747767,0.0011019758107579944,0.0001587718911722378,0.00029161240487343476,0.023774400373556132,0.0044547698399390843,0.001372623539863253,0.0090904836788511106,6.3601135172150482e-06,0.99973845989494048,2.4552821804945914e-05,0.021732651155699299,0.99982182024230593,0.99984354833540134,0.99744436066869846,8.9989444663975531e-05,3.7778185556917635e-06,0.99997553514464321,0.00067783650979909803,0.00041191882277806995,0.0050496785530249272,0.99999933692064391,0.012546232690983443,0.014103604670684469,0.00043470779617807518,0.99999932959264359,0.99835930918194682,0.0025582934759558634,0.12709933152061567,0.99676094350262967,0.9833337325224083,6.9235778881309462e-05,0.93278095717935394,0.048899492572318445,0.91572646085628684,0.26231270573386523,0.012079812579912249,0.00033142215555053431,0.40915925473353232,0.87256176682337949,0.99934408759309046,0.0011974120550461449,0.00079629398434483752,0.89118761320262452,0.94415399972260761,0.99609126241652002,6.69325770237538e-05,0.00082911004754191624,0.002881368897839996,0.0017846030462786338,0.00076503923079844591,2.87341679526324e-06,3.8965990441351659e-05,0.95823462386235581,0.5040658355929899,0.99973967717072032,0.9999722038690072,0.0010492121203519843,0.97159380299331155,0.94806378084103504,0.0082988346485232505,0.0042790652228158034,0.34092411529972177,0.00012625904134048278,0.00028440922633490178,0.99997909217582026,0.00047980611565371056,0.0018004798476501307,0.99999582677410637,0.74093313103187652,0.030953852350013321,0.014746076810555455,0.00033357206718350553,0.0096545400760161808,0.99535644062420692,0.99251922871018583,0.96126841290298304,0.013111641004797222,9.9223647506071624e-07,0.99999727434573704,0.0012331270819546417,0.0010901312531541277,0.022118752212858351,3.5870396105639534e-05,0.99947358263883412,0.001324899622030971,0.99999976957577241,0.99915314059995608,0.0074253806275745996,0.00049504690665614129,0.03997419203936723,0.028009469676190751,0.045849207277916228,0.090815582137629816,0.99713530950094931,0.0087740321902465244,0.99999968380429438,0.99689496461488691,0.00028836783398084628,0.001772355220526106,0.45706390398532365,0.57859012934015497,0.0014554592433055141,0.0026457962736066861,0.99553943054857719,0.0018202893948287477,0.00059112433197930846,0.00036250592960833638,3.0790540845672001e-05,0.99999806228423704,8.9640057315764726e-06,0.99999787981238353,0.9996412130899206,0.99991692048426917,0.00081626354708174938,0.013552896352097932,6.4886446899741995e-06,0.00026369249809228101,1.1130584183612732e-07,0.0042608129416332914,0.0011502334142987,1.8262585398574964e-06,0.98878648267574631,0.86614512691255519,7.01553181649738e-05,0.0012606073776191568,3.0315699065442381e-05,0.27242678089869948,0.17739435943512302,0.99998355251200965,0.00028440922633490178,0.99303164717284531,5.0924160768164907e-06,0.00019389483785481831,0.029000194421931892,0.99985502079134347,0.00051669519286773877,0.99995516374662963,3.7181286730171773e-05,0.12786671084907283,0.0027001263816906467,0.00061540202446684123,0.10914417668719828,0.00019825454756834999,0.026754892253535365,1.3189731408195604e-05,0.99524180883524671,0.88428616618545175,0.99995683477809993,0.29803239897585726,0.026984649886865223,0.0019003792818226739,0.0024851474680908747,0.0041385990217830569,6.1328525300673314e-05,0.99994622856928528,2.8922196085978874e-05,0.045077852937048891,0.00012265814096523993,0.99998359201971809,8.0105660752544238e-05,0.011034808635005571,0.99828481113350187,0.99939895815811008,2.992188879146047e-06,0.42719484557893322,0.0067477527548032781,0.26101869185272308,0.003602848302585521,0.99999833979515551,0.00056503217303786706,5.8394230014411791e-06,0.013174026557107322,0.19017719453630735,0.00095505938651263274,0.18983358399887262,0.085941470536854969,0.99762640497296839,0.018464395284543383,0.0011869830472102157,0.97502954110509554,0.0079743174081624481,2.0341874859758787e-06,0.0024304808904847901,6.7529079488795032e-05,0.99972156085602804,0.99957164311569224,0.015787484366577113,0.99770963899072651,0.20326334646760974,7.1302115311237773e-06,0.0024201638965907863,0.9998407430005416,1.8389014027178741e-06,0.023407796251840553,0.00020589866357521993,2.4214986433855513e-05,0.0053733720102236036,0.99999734016410868,0.37231494492210748,0.9991081226482893,0.77655885885668718,0.015618494795570865,0.5802433622197114,0.0059572199248790403,0.017943635993395358,0.00067319868260557727],"x":[594,446,674,525,657,918,318,364,342,387,527,716,557,450,344,372,258,338,410,937,702,469,535,342,819,736,330,274,341,717,478,487,239,825,400,728,773,425,943,510,389,270,945,497,329,389,475,383,432,619,578,411,445,440,359,419,840,393,754,441,803,444,753,688,431,511,464,473,532,280,342,320,531,373,547,611,825,431,401,517,803,586,444,693,659,416,423,756,245,419,757,617,909,516,317,425,528,416,645,390,393,394,387,450,487,607,369,489,324,417,694,651,395,442,422,404,381,501,944,753,591,735,538,451,477,436,738,902,464,944,285,453,382,414,335,935,203,348,800,436,360,674,425,901,453,350,362,486,471,459,506,262,825,291,464,802,818,736,364,308,862,349,375,423,938,456,517,373,898,777,470,545,699,697,300,677,497,669,596,492,346,590,592,780,432,418,662,678,716,330,414,416,403,362,284,363,655,597,794,818,409,681,606,489,475,590,396,420,857,371,421,828,594,533,462,392,475,752,659,650,496,211,898,388,383,455,319,756,377,940,757,469,394,484,491,547,519,739,479,943,742,357,432,584,595,401,460,753,466,362,361,338,882,293,922,793,787,400,516,295,307,151,441,406,270,680,662,347,453,309,592,540,886,420,718,284,323,513,841,362,842,321,516,428,383,521,358,489,252,720,610,871,594,522,379,454,450,317,835,297,516,355,858,305,410,707,798,265,576,448,590,456,930,412,286,440,546,385,544,505,732,506,394,674,458,251,429,348,789,795,509,754,580,289,390,787,241,522,412,359,489,940,592,796,653,459,586,401,500,373],"y":[3.9399999999999999,4.0599999999999996,3.8300000000000001,3.6200000000000001,4.4000000000000004,4.54,3.0899999999999999,4.8899999999999997,3.7400000000000002,3,2.4300000000000002,3.1600000000000001,3.5099999999999998,3.21,3.02,3.8700000000000001,2.4900000000000002,2.6600000000000001,3.1400000000000001,5,3.5299999999999998,4.2400000000000002,4.4699999999999998,3.6000000000000001,4.4500000000000002,3.9399999999999999,2.54,4.0599999999999996,4.4699999999999998,2.98,3.48,3.7400000000000002,2.4700000000000002,3.3199999999999998,3.5299999999999998,2.6600000000000001,4.8899999999999997,3.6200000000000001,4.4000000000000004,2.5600000000000001,3.3399999999999999,2.5600000000000001,4.3099999999999996,3.02,2.8599999999999999,2.98,3.3900000000000001,2.3599999999999999,2.3300000000000001,1.9399999999999999,4.1699999999999999,3.0699999999999998,3,3.6200000000000001,3.9199999999999999,3.8500000000000001,5,4.4900000000000002,3.7400000000000002,4.75,4.8899999999999997,4.1500000000000004,5,4.29,4.29,3.7400000000000002,2.2200000000000002,3.5699999999999998,3.7400000000000002,3.4100000000000001,3.71,2.1499999999999999,3.4100000000000001,2.0099999999999998,4.4000000000000004,4.0300000000000002,4.6600000000000001,3.6200000000000001,3.6899999999999999,4.2000000000000002,4.1500000000000004,5,3.21,3.7999999999999998,4.2000000000000002,3.8700000000000001,2.75,3.5499999999999998,2.52,3.7599999999999998,3.1099999999999999,4.3300000000000001,3.21,2.4700000000000002,1.51,3.5299999999999998,4.6299999999999999,3.3700000000000001,4.0800000000000001,3.1600000000000001,3.7599999999999998,3.0699999999999998,3.8700000000000001,3.6200000000000001,3.46,2.4900000000000002,2.2200000000000002,4.9800000000000004,3.0499999999999998,4.4699999999999998,1.8999999999999999,5,3.46,2.29,4.54,4.0599999999999996,3.3700000000000001,4.7699999999999996,5,4.4299999999999997,4.9299999999999997,4.0300000000000002,3.0499999999999998,4.4900000000000002,3.8700000000000001,4.1299999999999999,3.0499999999999998,5,3.8999999999999999,3.9199999999999999,3.5299999999999998,4.6799999999999997,3.5099999999999998,2.0299999999999998,3.71,5,2.7200000000000002,5,4.2400000000000002,3.5099999999999998,3.23,4.4699999999999998,2.4300000000000002,2.7000000000000002,4.9800000000000004,3,2.8900000000000001,3.4100000000000001,4.3799999999999999,5,5,2.7000000000000002,4.9500000000000002,2.54,2.7000000000000002,3.7799999999999998,4.2400000000000002,3.7799999999999998,4.0099999999999998,4.8200000000000003,4.1699999999999999,1.6699999999999999,3.0499999999999998,2.54,3.6899999999999999,2.9100000000000001,5,2.9300000000000002,2.2599999999999998,4.8600000000000003,4.8399999999999999,3.9399999999999999,2.6600000000000001,4.0599999999999996,1.9399999999999999,4.6299999999999999,3.1400000000000001,4.5599999999999996,4.9800000000000004,4.2400000000000002,2.2000000000000002,4.1699999999999999,2.2000000000000002,4.1500000000000004,4.1500000000000004,4.0099999999999998,4.5599999999999996,4.4900000000000002,3.4399999999999999,3.0499999999999998,3.8300000000000001,2.79,2.75,2.0299999999999998,4.2000000000000002,4.7199999999999998,3.3900000000000001,4.0800000000000001,3.8300000000000001,2.7000000000000002,3.4399999999999999,3.9700000000000002,1.8300000000000001,4.4699999999999998,4.5599999999999996,4.4299999999999997,4.8600000000000003,5,3.8500000000000001,2.77,3.3900000000000001,1.3700000000000001,3.0499999999999998,4.8600000000000003,2.98,3.8500000000000001,3.8300000000000001,4.8899999999999997,1.97,3.1400000000000001,4.3099999999999996,2.52,3.5099999999999998,2.54,2.4700000000000002,2.3599999999999999,3.21,3.0899999999999999,2.0800000000000001,2.8199999999999998,3.5499999999999998,3.8500000000000001,3.5699999999999998,2.8599999999999999,3.4399999999999999,5,3.3399999999999999,3.9900000000000002,4.0599999999999996,3.21,4.1699999999999999,2.7200000000000002,3.7999999999999998,3.7799999999999998,3.7400000000000002,2.8599999999999999,4.4500000000000002,4.8899999999999997,5,2.2599999999999998,2.6600000000000001,4.0300000000000002,2.6299999999999999,3.5099999999999998,4.1500000000000004,4.0800000000000001,2.5600000000000001,3.3399999999999999,5,3.8700000000000001,1,2.3100000000000001,3.3399999999999999,3.25,4.0999999999999996,3.0899999999999999,4.7699999999999996,3.6200000000000001,4.8600000000000003,3,4.79,3.4100000000000001,4.6799999999999997,5,4.0300000000000002,3.6899999999999999,1.8500000000000001,4.2000000000000002,5,2.3799999999999999,3.9900000000000002,3.25,2.8900000000000001,3.2799999999999998,2.98,3.23,3.0899999999999999,3.4100000000000001,1.6899999999999999,3.7599999999999998,2.75,5,4.75,4.5899999999999999,1.8300000000000001,4.29,3.6899999999999999,2.6600000000000001,3.8999999999999999,2.6099999999999999,3.8999999999999999,3.4100000000000001,3.6699999999999999,1.99,1.3700000000000001,2.3799999999999999,4.7199999999999998,3.48,3.6000000000000001,3.1800000000000002,4.7699999999999996,4.0300000000000002,4.2199999999999998,4.0999999999999996,3.6400000000000001,2.29,3.5499999999999998,2.6600000000000001,3.48,2.8900000000000001,3.5699999999999998,4.3600000000000003,2.79,3.6000000000000001,3.3900000000000001,3.3199999999999998,3.4100000000000001,3.6899999999999999,3.71,4.3099999999999996,4.6100000000000003,4.3300000000000001,4.7000000000000002,3.5699999999999998,2.0099999999999998,3.1400000000000001,3.0499999999999998,4.7199999999999998,5,5,4.8600000000000003,5,4.3799999999999999,5,5,2.8199999999999998,3.4100000000000001,1.6000000000000001,4.1699999999999999,2.54],"type":"mesh3d","name":"Fitted values","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 5.8: 3D visualization of the fitted <code>simpler_model</code> against the <code>salespeople</code> data
</p>
</div>
<p>Now let’s look at the summary of our <code>simpler_model</code>.</p>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb311-1"><a href="bin-log-reg.html#cb311-1" tabindex="-1"></a><span class="fu">summary</span>(simpler_model)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = promoted ~ sales + customer_rate, family = &quot;binomial&quot;, 
##     data = salespeople)
## 
## Coefficients:
##                 Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   -19.517689   3.346762  -5.832 5.48e-09 ***
## sales           0.040389   0.006525   6.190 6.03e-10 ***
## customer_rate  -1.122064   0.466958  -2.403   0.0163 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 440.303  on 349  degrees of freedom
## Residual deviance:  65.131  on 347  degrees of freedom
## AIC: 71.131
## 
## Number of Fisher Scoring iterations: 8</code></pre>
<p>Note that, unlike what we saw for linear regression in Section <a href="linear-reg-ols.html#lin-good-fit">4.3.3</a>, our summary does not provide a statistic on overall model fit or goodness-of-fit. The main reason for this is that there is no clear unified point of view in the statistics community on a single appropriate measure for model fit in the case of logistic regression. Nevertheless, a number of options are available to analysts for estimating fit and goodness-of-fit for these models.</p>
<p><em>Pseudo-<span class="math inline">\(R^2\)</span></em> measures are attempts to estimate the amount of variance in the outcome that is explained by the fitted model, analogous to the <span class="math inline">\(R^2\)</span> in linear regression. There are numerous variants of pseudo-<span class="math inline">\(R^2\)</span> with some of the most common listed here:</p>
<ul>
<li>McFadden’s <span class="math inline">\(R^2\)</span> works by comparing the likelihood function of the fitted model with that of a random model and using this to estimate the explained variance in the outcome.</li>
<li>Cox and Snell’s <span class="math inline">\(R^2\)</span> works by applying a ‘sum of squares’ analogy to the likelihood functions to align more closely with the precise methodology for calculating <span class="math inline">\(R^2\)</span> in linear regression. However, this usually means that the maximum value is less than 1 and in certain circumstances substantially less than 1, which can be problematic and unintuitive for an <span class="math inline">\(R^2\)</span>.</li>
<li>Nagelkerke’s <span class="math inline">\(R^2\)</span> resolves the issue with the upper bound for Cox and Snell by dividing Cox and Snell’s <span class="math inline">\(R^2\)</span> by its upper bound. This restores an intuitive scale with a maximum of 1, but is considered somewhat arbitrary with limited theoretical foundation.</li>
<li>Tjur’s <span class="math inline">\(R^2\)</span> is a more recent and simpler concept. It is defined as simply the absolute difference between the predicted probabilities of the positive observations and those of the negative observations.</li>
</ul>
<p>Standard modeling functions generally do not offer the calculation of pseudo-<span class="math inline">\(R^2\)</span> as standard, but numerous methods are available for their calculation. For example:</p>
<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb313-1"><a href="bin-log-reg.html#cb313-1" tabindex="-1"></a><span class="fu">library</span>(DescTools)</span>
<span id="cb313-2"><a href="bin-log-reg.html#cb313-2" tabindex="-1"></a>DescTools<span class="sc">::</span><span class="fu">PseudoR2</span>(</span>
<span id="cb313-3"><a href="bin-log-reg.html#cb313-3" tabindex="-1"></a>  simpler_model, </span>
<span id="cb313-4"><a href="bin-log-reg.html#cb313-4" tabindex="-1"></a>  <span class="at">which =</span> <span class="fu">c</span>(<span class="st">&quot;McFadden&quot;</span>, <span class="st">&quot;CoxSnell&quot;</span>, <span class="st">&quot;Nagelkerke&quot;</span>, <span class="st">&quot;Tjur&quot;</span>)</span>
<span id="cb313-5"><a href="bin-log-reg.html#cb313-5" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>##   McFadden   CoxSnell Nagelkerke       Tjur 
##  0.8520759  0.6576490  0.9187858  0.8784834</code></pre>
<p>We see that the Cox and Snell variant is notably lower than the other estimates, which is consistent with the known issues with its upper bound. However, the other estimates are reasonably aligned and suggest a strong fit.</p>
<p>Goodness-of-fit tests for logistic regression models compare the predictions to the observed outcome and test the null hypothesis that they are similar. This means that, unlike in linear regression, a low p-value indicates a poor fit. One commonly used method is the Hosmer-Lemeshow test, which divides the observations into a number of groups (usually 10) according to their fitted probabilities, calculates the proportion of each group that is positive and then compares this to the expected proportions based on the model prediction using a Chi-squared test. However, this method has limitations. It is particularly problematic for situations where there is a low sample size and can return highly varied results based on the number of groups used. It is therefore recommended to use a range of goodness-of-fit tests, and not rely entirely on any one specific approach.</p>
<p>In R, the <code>generalhoslem</code> package can perform the popular Hosmer-Lemeshow test of goodness of fit for logistic regression models, and is recommended for exploration. Here is an example using the <code>logitgof()</code> function for assessing goodness-of-fit, which uses 10 groups as default.</p>
<div class="sourceCode" id="cb315"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb315-1"><a href="bin-log-reg.html#cb315-1" tabindex="-1"></a><span class="fu">library</span>(generalhoslem)</span>
<span id="cb315-2"><a href="bin-log-reg.html#cb315-2" tabindex="-1"></a></span>
<span id="cb315-3"><a href="bin-log-reg.html#cb315-3" tabindex="-1"></a><span class="co"># run Hosmer-Lemeshow GOF test on observed versus fitted values</span></span>
<span id="cb315-4"><a href="bin-log-reg.html#cb315-4" tabindex="-1"></a>simpler_model_diagnostics <span class="ot">&lt;-</span> generalhoslem<span class="sc">::</span><span class="fu">logitgof</span>(</span>
<span id="cb315-5"><a href="bin-log-reg.html#cb315-5" tabindex="-1"></a>  salespeople<span class="sc">$</span>promoted, </span>
<span id="cb315-6"><a href="bin-log-reg.html#cb315-6" tabindex="-1"></a>  <span class="fu">fitted</span>(simpler_model) </span>
<span id="cb315-7"><a href="bin-log-reg.html#cb315-7" tabindex="-1"></a>)</span>
<span id="cb315-8"><a href="bin-log-reg.html#cb315-8" tabindex="-1"></a></span>
<span id="cb315-9"><a href="bin-log-reg.html#cb315-9" tabindex="-1"></a><span class="co"># view results</span></span>
<span id="cb315-10"><a href="bin-log-reg.html#cb315-10" tabindex="-1"></a>simpler_model_diagnostics</span></code></pre></div>
<pre><code>## 
##  Hosmer and Lemeshow test (binary model)
## 
## data:  salespeople$promoted, fitted(simpler_model)
## X-squared = 3.4458, df = 8, p-value = 0.9034</code></pre>
<p>The non-significant result of the Hosmer-Lemeshow test suggests a good fit for our model.</p>
<p>Various measures of predictive accuracy can also be used to assess a binomial logistic regression model in a predictive context, such as precision, recall and ROC-curve analysis. These are particularly suited for implementations of logistic regression models as predictive classifiers in a Machine Learning context, a topic which is outside the scope of this book. However, a recommended source for a deeper treatment of goodness-of-fit tests for logistic regression models is <span class="citation">Hosmer, Lemeshow, and Sturdivant (<a href="#ref-hosmer-logistic">2013</a>)</span>.</p>
</div>
<div id="model-parsimony" class="section level3 hasAnchor" number="5.3.3">
<h3><span class="header-section-number">5.3.3</span> Model parsimony<a href="bin-log-reg.html#model-parsimony" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We saw that in both our linear regression and our logistic regression approach, we decided to drop variables from our model when we determined that they had no significant effect on the outcome. The principle of <em>Occam’s Razor</em> states that—all else being equal—the simplest explanation is the best. In this sense, a model that contains information that does not contribute to its primary inference objective is more complex than it needs to be. Such a model increases the communication burden in explaining its results to others, with no notable analytic benefit in return.</p>
<p><em>Parsimony</em> describes the concept of being careful with resources or with information. A model could be described as more parsimonious if it can achieve the same (or very close to the same) fit with a smaller number of inputs. The <em>Akaike Information Criterion</em> or <em>AIC</em> is a measure of model parsimony that is computed for log-likelihood models like logistic regression models, with a lower AIC indicating a more parsimonious model. AIC is often calculated as standard in summary reports of logistic regression models but can also be calculated independently. Let’s compare the different iterations of our model in this chapter using AIC.</p>
<div class="sourceCode" id="cb317"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb317-1"><a href="bin-log-reg.html#cb317-1" tabindex="-1"></a><span class="co"># sales only model</span></span>
<span id="cb317-2"><a href="bin-log-reg.html#cb317-2" tabindex="-1"></a><span class="fu">AIC</span>(sales_model)</span></code></pre></div>
<pre><code>## [1] 76.49508</code></pre>
<div class="sourceCode" id="cb319"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb319-1"><a href="bin-log-reg.html#cb319-1" tabindex="-1"></a><span class="co"># sales and customer rating model</span></span>
<span id="cb319-2"><a href="bin-log-reg.html#cb319-2" tabindex="-1"></a><span class="fu">AIC</span>(simpler_model)</span></code></pre></div>
<pre><code>## [1] 71.13145</code></pre>
<div class="sourceCode" id="cb321"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb321-1"><a href="bin-log-reg.html#cb321-1" tabindex="-1"></a><span class="co"># model with all inputs</span></span>
<span id="cb321-2"><a href="bin-log-reg.html#cb321-2" tabindex="-1"></a><span class="fu">AIC</span>(full_model)</span></code></pre></div>
<pre><code>## [1] 76.37433</code></pre>
<p>We can see that the model which is limited to our two significant inputs—sales and customer rating—is determined to be the most parsimonious model according to the AIC. Note that the AIC should not be used to interpret model quality or confidence—it is possible that the lowest AIC might still be a very poor fit.</p>
<p>Model parsimony becomes a substantial concern when there is a large number of input variables. As a general rule, the more input variables there are in a model the greater the chance that the model will be difficult to interpret clearly, and the greater the risk of measurement problems, such as multicollinearity. Analysts who are eager to please their customers, clients, professors or bosses can easily be tempted to think up new potential inputs to their model, often derived mathematically from measures that are already inputs in the model. Before long the model is too complex, and in extreme cases there are more inputs than there are observations. The primary way to manage model complexity is to exercise caution in selecting model inputs. When large numbers of inputs are unavoidable, coefficient regularization methods such as LASSO regression can help with model parsimony.</p>
</div>
</div>
<div id="other-considerations-in-binomial-logistic-regression" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Other considerations in binomial logistic regression<a href="bin-log-reg.html#other-considerations-in-binomial-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To predict from new data, just use the <code>predict()</code> function as in the previous chapter. This function recognizes the type of model being used—in this case a generalized linear model—and adjusts its prediction approach accordingly. In particular, if you want to return the probability of the new observations being promoted, you need to use <code>type = "response"</code> as an argument.</p>
<div class="sourceCode" id="cb323"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb323-1"><a href="bin-log-reg.html#cb323-1" tabindex="-1"></a><span class="co"># define new observations</span></span>
<span id="cb323-2"><a href="bin-log-reg.html#cb323-2" tabindex="-1"></a>(new_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">sales =</span> <span class="fu">c</span>(<span class="dv">420</span>, <span class="dv">510</span>, <span class="dv">710</span>), </span>
<span id="cb323-3"><a href="bin-log-reg.html#cb323-3" tabindex="-1"></a>                        <span class="at">customer_rate =</span> <span class="fu">c</span>(<span class="fl">3.4</span>, <span class="fl">2.3</span>, <span class="fl">4.2</span>)))</span></code></pre></div>
<pre><code>##   sales customer_rate
## 1   420           3.4
## 2   510           2.3
## 3   710           4.2</code></pre>
<div class="sourceCode" id="cb325"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb325-1"><a href="bin-log-reg.html#cb325-1" tabindex="-1"></a><span class="co"># predict probability of promotion</span></span>
<span id="cb325-2"><a href="bin-log-reg.html#cb325-2" tabindex="-1"></a><span class="fu">predict</span>(simpler_model, new_data, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<pre><code>##          1          2          3 
## 0.00171007 0.18238565 0.98840506</code></pre>
<p>Many of the principles covered in the previous chapter on linear regression are equally important in logistic regression. For example, input variables should be managed in a similar way. Collinearity and multicollinearity should be of concern. Interaction of input variables can be modeled. For the most part, analysts should be aware of the fundamental mathematical transformations which take place in a logistic regression model when they consider some of these issues (another reason to ensure that the mathematics covered earlier in this chapter is well understood). For example, while coefficients in linear regression have a direct additive impact on <span class="math inline">\(y\)</span>, in logistic regression they have a direct additive impact on the log odds of <span class="math inline">\(y\)</span>, or alternatively their exponents have a direct multiplicative impact on the odds of <span class="math inline">\(y\)</span>. Therefore coefficient overestimation such as that which can occur when collinearity is not managed can result in inferences that could substantially overstate the importance or effect of input variables.</p>
<p>Because of the binary nature of our outcome variable, the residuals of a logistic regression model have limited direct application to the problem being studied. In practical contexts the residuals of logistic regression models are rarely examined, but they can be useful in identifying outliers or particularly influential observations and in assessing goodness-of-fit. When residuals are examined, they need to be transformed in order to be analyzed appropriately. For example, the <em>Pearson residual</em> is a standardized form of residual from logistic regression which can be expected to have a normal distribution over large-enough samples. We can see in Figure <a href="bin-log-reg.html#fig:pearson-resids">5.9</a> that this is the case for our <code>simpler_model</code>, but that there are a small number of substantial underestimates in our model. A good source of further learning on diagnostics of logistic regression models is <span class="citation">Menard (<a href="#ref-menard">2010</a>)</span>.</p>
<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb327-1"><a href="bin-log-reg.html#cb327-1" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">density</span>(<span class="fu">residuals</span>(simpler_model, <span class="st">&quot;pearson&quot;</span>))</span>
<span id="cb327-2"><a href="bin-log-reg.html#cb327-2" tabindex="-1"></a><span class="fu">plot</span>(d, <span class="at">main=</span> <span class="st">&quot;&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pearson-resids"></span>
<img src="_main_files/figure-html/pearson-resids-1.png" alt="Distribution of Pearson residuals in `simpler_model`" width="672" />
<p class="caption">
Figure 5.9: Distribution of Pearson residuals in <code>simpler_model</code>
</p>
</div>
</div>
<div id="learning-exercises-3" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Learning exercises<a href="bin-log-reg.html#learning-exercises-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="discussion-questions-3" class="section level3 hasAnchor" number="5.5.1">
<h3><span class="header-section-number">5.5.1</span> Discussion questions<a href="bin-log-reg.html#discussion-questions-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><p>Draw the shape of a logistic function. Describe the three population growth phases it was originally intended to model.</p></li>
<li><p>Explain why the logistic function is useful to statisticians in modeling.</p></li>
<li><p>In the formula for the logistic function in Section <a href="bin-log-reg.html#logistic-origins">5.1.1</a>, what might be a common value for <span class="math inline">\(L\)</span> in probabilistic applications? Why?</p></li>
<li><p>What types of problems are suitable for logistic regression modeling?</p></li>
<li><p>Can you think of some modeling scenarios in your work or studies that could use a logistic regression approach?</p></li>
<li><p>Explain the concept of odds. How do odds differ from probability? How do odds change as probability increases?</p></li>
<li><p>Complete the following:</p></li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>If an event has a 1% probability of occurring, a 10% increase in odds results in an almost __% increase in probability.</li>
<li>If an event has a 99% probability of occurring, a 10% increase in odds results in an almost __% increase in probability.</li>
</ol>
<ol start="8" style="list-style-type: decimal">
<li><p>Describe how the coefficients of a logistic regression model affect the fitted outcome. If <span class="math inline">\(\beta\)</span> is a coefficient estimate, how is the odds ratio associated with <span class="math inline">\(\beta\)</span> calculated and what does it mean?</p></li>
<li><p>What are some of the options for determining the fit of a binomial logistic regression model?</p></li>
<li><p>Describe the concept of model parsimony. What measure is commonly used to determine the most parsimonious logistic regression model?</p></li>
</ol>
</div>
<div id="data-exercises-3" class="section level3 hasAnchor" number="5.5.2">
<h3><span class="header-section-number">5.5.2</span> Data exercises<a href="bin-log-reg.html#data-exercises-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A nature preservation charity has asked you to analyze some data to help them understand the features of those members of the public who donated in a given month. Load the <code>charity_donation</code> data set via the <code>peopleanalyticsdata</code> package or download it from the internet<a href="#fn28" class="footnote-ref" id="fnref28"><sup>28</sup></a>. It contains the following data:</p>
<ul>
<li><code>n_donations</code>: The total number of times the individual donated previous to the month being studied.</li>
<li><code>total_donations</code>: The total amount of money donated by the individual previous to the month being studied</li>
<li><code>time_donating</code>: The number of months between the first donation and the month being studied</li>
<li><code>recent_donation</code>: Whether or not the individual donated in the month being studied</li>
<li><code>last_donation</code>: The number of months between the most recent previous donation and the month being studied</li>
<li><code>gender</code>: The gender of the individual</li>
<li><code>reside</code>: Whether the person resides in an Urban or Rural Domestic location or Overseas</li>
<li><code>age</code>: The age of the individual</li>
</ul>
<ol style="list-style-type: decimal">
<li>View the data and obtain statistical summaries. Ensure data types are appropriate and there is no missing data. Determine the outcome and input variables.</li>
<li>Using a pairplot or by plotting or correlating selected fields, try to hypothesize which variables may be significant in explaining who recently donated.</li>
<li>Run a binomial logistic regression model using all input fields. Determine which input variables have a significant effect on the outcome and the direction of that effect.</li>
<li>Calculate the odds ratios for the significant variables and explain their impact on the outcome.</li>
<li>Check for collinearity or multicollinearity in your model using methods from previous chapters.</li>
<li>Experiment with model parsimony by reducing input variables that do not have a significant impact on the outcome. Decide on the most parsimonious model.</li>
<li>Calculate a variety of Pseudo-<span class="math inline">\(R^2\)</span> variants for your model. How would you explain these to someone with no statistics expertise?</li>
<li>Report the conclusions of your modeling exercise to the charity by writing a simple explanation that assumes no knowledge of statistics.</li>
<li><strong>Extension:</strong> Using a variety of methods of your choice, test the hypothesis that your model fits the data. How conclusive are your tests?</li>
</ol>

</div>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-hosmer-logistic" class="csl-entry">
Hosmer, David W., Stanley Lemeshow, and Rodney X. Sturdivant. 2013. <em>Applied Logistic Regression</em>.
</div>
<div id="ref-menard" class="csl-entry">
Menard, Scott. 2010. <em>Logistic Regression: From Introductory to Advanced Concepts and Applications</em>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="24">
<li id="fn24"><p>The logistic function plotted in Figure <a href="bin-log-reg.html#fig:norm-log-curves">5.2</a> takes the simple form <span class="math inline">\(y = \frac{1}{1 + e^{-x}}\)</span>.<a href="bin-log-reg.html#fnref24" class="footnote-back">↩︎</a></p></li>
<li id="fn25"><p>Often in sports the odds are expressed in the reverse order, but the concept is the same.<a href="bin-log-reg.html#fnref25" class="footnote-back">↩︎</a></p></li>
<li id="fn26"><p>In this case, a more general form of the Ordinary Least Squares procedure is used to fit the model, known as <em>maximum likelihood estimation</em>.<a href="bin-log-reg.html#fnref26" class="footnote-back">↩︎</a></p></li>
<li id="fn27"><p>Note that most standard modeling functions have a built-in capability to deal with categorical variables, meaning that it’s often not necessary to explicitly construct dummies. However, it is shown here for completion sake. You may wish to try running the subsequent code without explicitly constructing dummies, but note that constructing your own dummies gives you greater control over how they are labeled in any modeling output.<a href="bin-log-reg.html#fnref27" class="footnote-back">↩︎</a></p></li>
<li id="fn28"><p><a href="http://peopleanalytics-regression-book.org/data/charity_donation.csv" class="uri">http://peopleanalytics-regression-book.org/data/charity_donation.csv</a><a href="bin-log-reg.html#fnref28" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-reg-ols.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multinomial-logistic-regression-for-nominal-category-outcomes.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
